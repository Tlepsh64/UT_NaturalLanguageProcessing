{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldZKvx0wK-pU"
   },
   "source": [
    "# Homework 1: NLP Basics and NLP Pipelines (7 + 1 points)\n",
    "\n",
    "**Welcome to homework 1!** \n",
    "\n",
    "The homework contains several tasks. You can find the amount of points you get for the correct solution in the task header. Maximum amount of points for each homework is 7 + 1 (bonus exercise). \n",
    "The **grading** for each task is the following: \n",
    "* correct answer - **full points** \n",
    "* insufficient solution or solution resulting in the incorrect output - **half points**\n",
    "* no answer or completely wrong solution - **no points**\n",
    "\n",
    "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
    "\n",
    "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
    "\n",
    "When writing code, make it readable. Choose appropriate names for your variables (a = 'cat' - not good, word = 'cat' - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
    "\n",
    "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
    "\n",
    "\n",
    "\n",
    "**Before sending your solution, do the Kernel -> Restart & Run All to ensure that all your code works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stanza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sBVo1tJOK7Ft"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import stanza\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SxjgwPjeMlOg"
   },
   "outputs": [],
   "source": [
    "stanza.download(lang='en', verbose=False) # download appropriate language model for your chosen language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoO93eEzL3Dv"
   },
   "source": [
    "## Task 1: Find the data (0.5 points)\n",
    "\n",
    "Find large enough text data in English or any other language supported by spaCy and Stanza. If the resources for your language are very limited, you may use English or other language of your preference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkFhvibAMHUc"
   },
   "source": [
    "**What is the language of your data?**\n",
    "\n",
    "<font color='red'>English</font>\n",
    "\n",
    "**Where did you get the text data?**\n",
    "\n",
    "<font color='red'>Wikipedia. Here's the link: https://en.wikipedia.org/wiki/The_Metropolis_and_Mental_Life</font>\n",
    "\n",
    "**What kind of text is it? (books, magazines, news articles, etc.)**\n",
    "\n",
    "<font color='red'>It is an info page for Georg Simmel's sociological analysis of modern city, 'The Metropolis and Mental Life' .</font>\n",
    "\n",
    "**What style(s) of text does your data have? (user commetaries, scientific, neutral, etc.)**\n",
    "\n",
    "<font color='red'>It has a high number of html tags besides the actual text data. It is organized in a couple of different sections(sub-headings named as 'overview', 'references', 'see also' etc), includes a table of contents, footer and a header. In short it is a classical wikipedia webpage, it is hard to describe everything in it with words.</font>\n",
    "\n",
    "**Was it easy to download the data? If no, desribe what difficulties you had and how you resolved them.**\n",
    "\n",
    "<font color='red'>Yes, it was easy to download it with a few clicks.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExUcWs0BM44Y"
   },
   "source": [
    "## Task 2: Tokenize and count statistics (1 points)\n",
    "\n",
    "Using either NLTK or Spacy tools, tokenize your text data you found in the previous exercise. \n",
    "\n",
    "P.S. If you are using Spacy, don't forget to load an appropriate module for it. \n",
    "\n",
    "**Compute and output the following:**\n",
    "* number of sentences\n",
    "* number of tokens\n",
    "* number of unique tokens (or types)\n",
    "* average length of a sentence \n",
    "* average length of a token\n",
    "* sentence length (tokens in a sentence) histogram (you can use matplotlib.pyplot for that) \n",
    "* token length (characters in a token) histogram (you can use matplotlib.pyplot for that) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8-3e0X3EM2zC"
   },
   "outputs": [],
   "source": [
    "# Replace the path with the name of your data file\n",
    "data_path = \"metropolis.html\"\n",
    "\n",
    "data = open(data_path, errors='ignore').read()\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "# Split the data into sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Metropolis and Mental Life (German: Die GroÃŸstÃ¤dte und das Geistesleben) is a 1903 essay by the German sociologist, Georg Simmel.\n",
      "One of Simmel's most widely read works, The Metropolis was originally provided as one of a series of lectures on all aspects of city life by experts in various fields, ranging from science and religion to art. The series was conducted alongside the Dresden cities exhibition of 1903. Simmel was originally asked to lecture on the role of intellectual (or scholarly) life in the big city, but he reversed the topic in order to analyze the effects of the big city on the mind of the individual.[citation needed] As a result, when the lectures were published as essays in a book, to fill the gap, the series editor had to supply an essay on the original topic himself.[citation needed]\n",
      "Simmel compared the psychology of the individual in rural life with the psychology of the city dweller. His investigation determines that the human psychology is altered by the metropolis. The individual must contend with such change in a metropolitan environment that the psychology of such an individual erects defences to protect itself from the stimuli of the metropolis. As such, the city dwellerâ€™s attitude and psychology is fundamentally different to an individual that inhabits rural life. The psychology of the city dweller, therefore, exhibits what Simmel describes as adaptations and adjustments which ultimately reflect the structures of the metropolis. Simmel characterises rural life as a combination of meaningful relationships, established over time. These kinds of relationships can not be established in the metropolis for a number of reasons (e.g. anonymity, number of vendors etc.), and as a result, the city dweller can only establish a relationship with currency â€“ money and exchange becomes the medium within which the city dweller invests their trust.[1]\n",
      "The deepest problems of modern life flow from the attempt of the individual to maintain the independence and individuality of his existence against the sovereign powers of society, against the weight of the historical heritage and the external culture and technique of life. The antagonism represents the most modern form of the conflict which primitive man must carry on with nature for his own bodily existence. The eighteenth century may have called for liberation from all the ties which grew up historically in politics, in religion, in morality and in economics in order to permit the original natural virtue of man, which is equal in everyone, to develop without inhibition; the nineteenth century may have sought to promote, in addition to man's freedom, his individuality (which is connected with the division of labor) and his achievements which make him unique and indispensable but which at the same time make him so much the more dependent on the complementary activity of others; Nietzsche may have seen the relentless struggle of the individual as the prerequisite for his full development, while socialism found the same thing in the suppression of all competition â€“ but in each of these the same fundamental motive was at work, namely the resistance of the individual to being levelled, swallowed up in the social-technological mechanism.Simmel seeks to explain human nature and how it plays a part in society.\n",
      "Man's nature, originally good and common to all, should develop unhampered. In addition to more liberty, the nineteenth century demanded the functional specialization of man and his work; this specialization makes one individual incomparable to another, and each of them indispensable to the highest possible extent.\n"
     ]
    }
   ],
   "source": [
    "# Let's get the raw data first.\n",
    "\n",
    "tag = soup.find_all(['p']) \n",
    "wikitext = ''\n",
    "\n",
    "for sents in tag:\n",
    "    wikitext += sents.text\n",
    "    \n",
    "print(wikitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print if you wanna get a better grasp of the location of \\n characters.\n",
    "# wikitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some patterns in the data(eg: '[citation needed]' ) are not needed, and paragraphs can be separeted better than in the raw text. For this end I will use some regex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Metropolis and Mental Life (German: Die GroÃŸstÃ¤dte und das Geistesleben) is a 1903 essay by the German sociologist, Georg Simmel.\n",
      "One of Simmel's most widely read works, The Metropolis was originally provided as one of a series of lectures on all aspects of city life by experts in various fields, ranging from science and religion to art. The series was conducted alongside the Dresden cities exhibition of 1903. Simmel was originally asked to lecture on the role of intellectual (or scholarly) life in the big city, but he reversed the topic in order to analyze the effects of the big city on the mind of the individual. As a result, when the lectures were published as essays in a book, to fill the gap, the series editor had to supply an essay on the original topic himself.\n",
      "Simmel compared the psychology of the individual in rural life with the psychology of the city dweller. His investigation determines that the human psychology is altered by the metropolis. The individual must contend with such change in a metropolitan environment that the psychology of such an individual erects defences to protect itself from the stimuli of the metropolis. As such, the city dwellerâ€™s attitude and psychology is fundamentally different to an individual that inhabits rural life. The psychology of the city dweller, therefore, exhibits what Simmel describes as adaptations and adjustments which ultimately reflect the structures of the metropolis. Simmel characterises rural life as a combination of meaningful relationships, established over time. These kinds of relationships can not be established in the metropolis for a number of reasons (e.g. anonymity, number of vendors etc.), and as a result, the city dweller can only establish a relationship with currency â€“ money and exchange becomes the medium within which the city dweller invests their trust.\n",
      "The deepest problems of modern life flow from the attempt of the individual to maintain the independence and individuality of his existence against the sovereign powers of society, against the weight of the historical heritage and the external culture and technique of life. The antagonism represents the most modern form of the conflict which primitive man must carry on with nature for his own bodily existence. The eighteenth century may have called for liberation from all the ties which grew up historically in politics, in religion, in morality and in economics in order to permit the original natural virtue of man, which is equal in everyone, to develop without inhibition; the nineteenth century may have sought to promote, in addition to man's freedom, his individuality (which is connected with the division of labor) and his achievements which make him unique and indispensable but which at the same time make him so much the more dependent on the complementary activity of others; Nietzsche may have seen the relentless struggle of the individual as the prerequisite for his full development, while socialism found the same thing in the suppression of all competition â€“ but in each of these the same fundamental motive was at work, namely the resistance of the individual to being levelled, swallowed up in the social-technological mechanism.\n",
      "Simmel seeks to explain human nature and how it plays a part in society.\n",
      "Man's nature, originally good and common to all, should develop unhampered. In addition to more liberty, the nineteenth century demanded the functional specialization of man and his work; this specialization makes one individual incomparable to another, and each of them indispensable to the highest possible extent.\n"
     ]
    }
   ],
   "source": [
    "simmeltext = re.sub(r\"\\[citation needed]|\\[\\d]\", \"\", wikitext)\n",
    "simmeltext = re.sub(r\"\\.S\", '.\\nS', simmeltext)\n",
    "\n",
    "print(simmeltext) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Metropolis and Mental Life (German: Die GroÃŸstÃ¤dte und das Geistesleben) is a 1903 essay by the German sociologist, Georg Simmel.\n",
      "\n",
      "One of Simmel's most widely read works, The Metropolis was originally provided as one of a series of lectures on all aspects of city life by experts in various fields, ranging from science and religion to art. The series was conducted alongside the Dresden cities exhibition of 1903. Simmel was originally asked to lecture on the role of intellectual (or scholarly) life in the big city, but he reversed the topic in order to analyze the effects of the big city on the mind of the individual. As a result, when the lectures were published as essays in a book, to fill the gap, the series editor had to supply an essay on the original topic himself.\n",
      "\n",
      "Simmel compared the psychology of the individual in rural life with the psychology of the city dweller. His investigation determines that the human psychology is altered by the metropolis. The individual must contend with such change in a metropolitan environment that the psychology of such an individual erects defences to protect itself from the stimuli of the metropolis. As such, the city dwellerâ€™s attitude and psychology is fundamentally different to an individual that inhabits rural life. The psychology of the city dweller, therefore, exhibits what Simmel describes as adaptations and adjustments which ultimately reflect the structures of the metropolis. Simmel characterises rural life as a combination of meaningful relationships, established over time. These kinds of relationships can not be established in the metropolis for a number of reasons (e.g. anonymity, number of vendors etc.), and as a result, the city dweller can only establish a relationship with currency â€“ money and exchange becomes the medium within which the city dweller invests their trust.\n",
      "\n",
      "The deepest problems of modern life flow from the attempt of the individual to maintain the independence and individuality of his existence against the sovereign powers of society, against the weight of the historical heritage and the external culture and technique of life. The antagonism represents the most modern form of the conflict which primitive man must carry on with nature for his own bodily existence. The eighteenth century may have called for liberation from all the ties which grew up historically in politics, in religion, in morality and in economics in order to permit the original natural virtue of man, which is equal in everyone, to develop without inhibition; the nineteenth century may have sought to promote, in addition to man's freedom, his individuality (which is connected with the division of labor) and his achievements which make him unique and indispensable but which at the same time make him so much the more dependent on the complementary activity of others; Nietzsche may have seen the relentless struggle of the individual as the prerequisite for his full development, while socialism found the same thing in the suppression of all competition â€“ but in each of these the same fundamental motive was at work, namely the resistance of the individual to being levelled, swallowed up in the social-technological mechanism.\n",
      "\n",
      "Simmel seeks to explain human nature and how it plays a part in society.\n",
      "\n",
      "Man's nature, originally good and common to all, should develop unhampered. In addition to more liberty, the nineteenth century demanded the functional specialization of man and his work; this specialization makes one individual incomparable to another, and each of them indispensable to the highest possible extent.\n"
     ]
    }
   ],
   "source": [
    "simmeltext = re.sub(r\"\\n\", '\\n\\n', simmeltext)\n",
    "simmeltext = re.sub(r'\\n\\n', '', simmeltext, 1)\n",
    "print(simmeltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cleaning can still be made to include the german characters, but I will stop the process here for practical reasons. The text above seems good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into sentences and tokens\n",
    "simmeldoc = nlp(simmeltext)\n",
    "\n",
    "# These lists below are subject to change. But let's create them first and check their elements later.\n",
    "simmel_sents_list = [sent.text for sent in simmeldoc.sents]\n",
    "simmel_tokens_list = [token.text for token in simmeldoc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Metropolis and Mental Life',\n",
       " '(German',\n",
       " ': Die GroÃŸstÃ¤dte',\n",
       " 'und das Geistesleben) is a 1903 essay by the German sociologist, Georg Simmel.\\n\\n',\n",
       " \"One of Simmel's most widely read works, The Metropolis was originally provided as one of a series of lectures on all aspects of city life by experts in various fields, ranging from science and religion to art.\",\n",
       " 'The series was conducted alongside the Dresden cities exhibition of 1903.',\n",
       " 'Simmel was originally asked to lecture on the role of intellectual (or scholarly) life in the big city, but he reversed the topic in order to analyze the effects of the big city on the mind of the individual.',\n",
       " 'As a result, when the lectures were published as essays in a book, to fill the gap, the series editor had to supply an essay on the original topic himself.\\n\\n',\n",
       " 'Simmel compared the psychology of the individual in rural life with the psychology of the city dweller.',\n",
       " 'His investigation determines that the human psychology is altered by the metropolis.',\n",
       " 'The individual must contend with such change in a metropolitan environment that the psychology of such an individual erects defences to protect itself from the stimuli of the metropolis.',\n",
       " 'As such, the city dwellerâ€™s attitude and psychology is fundamentally different to an individual that inhabits rural life.',\n",
       " 'The psychology of the city dweller, therefore, exhibits what Simmel describes as adaptations and adjustments which ultimately reflect the structures of the metropolis.',\n",
       " 'Simmel characterises rural life as a combination of meaningful relationships, established over time.',\n",
       " 'These kinds of relationships can not be established in the metropolis for a number of reasons (e.g. anonymity, number of vendors etc.), and as a result, the city dweller can only establish a relationship with currency',\n",
       " 'â€“ money and exchange becomes the medium within which the city dweller invests their trust.\\n\\n',\n",
       " 'The deepest problems of modern life flow from the attempt of the individual to maintain the independence and individuality of his existence against the sovereign powers of society, against the weight of the historical heritage and the external culture and technique of life.',\n",
       " 'The antagonism represents the most modern form of the conflict which primitive man must carry on with nature for his own bodily existence.',\n",
       " \"The eighteenth century may have called for liberation from all the ties which grew up historically in politics, in religion, in morality and in economics in order to permit the original natural virtue of man, which is equal in everyone, to develop without inhibition; the nineteenth century may have sought to promote, in addition to man's freedom, his individuality (which is connected with the division of labor) and his achievements which make him unique and indispensable but which at the same time make him so much the more dependent on the complementary activity of others; Nietzsche may have seen the relentless struggle of the individual as the prerequisite for his full development, while socialism found the same thing in the suppression of all competition â€“ but in each of these the same fundamental motive was at work, namely the resistance of the individual to being levelled, swallowed up in the social-technological mechanism.\\n\\n\",\n",
       " 'Simmel seeks to explain human nature and how it plays a part in society.\\n\\n',\n",
       " \"Man's nature, originally good and common to all, should develop unhampered.\",\n",
       " 'In addition to more liberty, the nineteenth century demanded the functional specialization of man and his work; this specialization makes one individual incomparable to another, and each of them indispensable to the highest possible extent.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmel_sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, spacy splitted the first sentence(the summary section of the wiki article) wrongly by creating 4 sentences out of it. We need to merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Metropolis and Mental Life(German: Die GroÃŸstÃ¤dteund das Geistesleben) is a 1903 essay by the German sociologist, Georg Simmel.\\n\\n',\n",
       " \"One of Simmel's most widely read works, The Metropolis was originally provided as one of a series of lectures on all aspects of city life by experts in various fields, ranging from science and religion to art.\",\n",
       " 'The series was conducted alongside the Dresden cities exhibition of 1903.',\n",
       " 'Simmel was originally asked to lecture on the role of intellectual (or scholarly) life in the big city, but he reversed the topic in order to analyze the effects of the big city on the mind of the individual.',\n",
       " 'As a result, when the lectures were published as essays in a book, to fill the gap, the series editor had to supply an essay on the original topic himself.\\n\\n',\n",
       " 'Simmel compared the psychology of the individual in rural life with the psychology of the city dweller.',\n",
       " 'His investigation determines that the human psychology is altered by the metropolis.',\n",
       " 'The individual must contend with such change in a metropolitan environment that the psychology of such an individual erects defences to protect itself from the stimuli of the metropolis.',\n",
       " 'As such, the city dwellerâ€™s attitude and psychology is fundamentally different to an individual that inhabits rural life.',\n",
       " 'The psychology of the city dweller, therefore, exhibits what Simmel describes as adaptations and adjustments which ultimately reflect the structures of the metropolis.',\n",
       " 'Simmel characterises rural life as a combination of meaningful relationships, established over time.',\n",
       " 'These kinds of relationships can not be established in the metropolis for a number of reasons (e.g. anonymity, number of vendors etc.), and as a result, the city dweller can only establish a relationship with currency',\n",
       " 'â€“ money and exchange becomes the medium within which the city dweller invests their trust.\\n\\n',\n",
       " 'The deepest problems of modern life flow from the attempt of the individual to maintain the independence and individuality of his existence against the sovereign powers of society, against the weight of the historical heritage and the external culture and technique of life.',\n",
       " 'The antagonism represents the most modern form of the conflict which primitive man must carry on with nature for his own bodily existence.',\n",
       " \"The eighteenth century may have called for liberation from all the ties which grew up historically in politics, in religion, in morality and in economics in order to permit the original natural virtue of man, which is equal in everyone, to develop without inhibition; the nineteenth century may have sought to promote, in addition to man's freedom, his individuality (which is connected with the division of labor) and his achievements which make him unique and indispensable but which at the same time make him so much the more dependent on the complementary activity of others; Nietzsche may have seen the relentless struggle of the individual as the prerequisite for his full development, while socialism found the same thing in the suppression of all competition â€“ but in each of these the same fundamental motive was at work, namely the resistance of the individual to being levelled, swallowed up in the social-technological mechanism.\\n\\n\",\n",
       " 'Simmel seeks to explain human nature and how it plays a part in society.\\n\\n',\n",
       " \"Man's nature, originally good and common to all, should develop unhampered.\",\n",
       " 'In addition to more liberty, the nineteenth century demanded the functional specialization of man and his work; this specialization makes one individual incomparable to another, and each of them indispensable to the highest possible extent.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Much better now.\n",
    "simmel_sents_list[0 : 4] = [''.join(simmel_sents_list[0 : 4])] \n",
    "simmel_sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Metropolis',\n",
       " 'and',\n",
       " 'Mental',\n",
       " 'Life',\n",
       " '(',\n",
       " 'German',\n",
       " ':',\n",
       " 'Die',\n",
       " 'GroÃŸstÃ¤dte',\n",
       " 'und',\n",
       " 'das',\n",
       " 'Geistesleben',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " '1903',\n",
       " 'essay',\n",
       " 'by',\n",
       " 'the',\n",
       " 'German',\n",
       " 'sociologist',\n",
       " ',',\n",
       " 'Georg',\n",
       " 'Simmel',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'One',\n",
       " 'of',\n",
       " 'Simmel',\n",
       " \"'s\",\n",
       " 'most',\n",
       " 'widely',\n",
       " 'read',\n",
       " 'works',\n",
       " ',',\n",
       " 'The',\n",
       " 'Metropolis',\n",
       " 'was',\n",
       " 'originally',\n",
       " 'provided',\n",
       " 'as',\n",
       " 'one',\n",
       " 'of',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'lectures',\n",
       " 'on',\n",
       " 'all',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'city',\n",
       " 'life',\n",
       " 'by',\n",
       " 'experts',\n",
       " 'in',\n",
       " 'various',\n",
       " 'fields',\n",
       " ',',\n",
       " 'ranging',\n",
       " 'from',\n",
       " 'science',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'to',\n",
       " 'art',\n",
       " '.',\n",
       " 'The',\n",
       " 'series',\n",
       " 'was',\n",
       " 'conducted',\n",
       " 'alongside',\n",
       " 'the',\n",
       " 'Dresden',\n",
       " 'cities',\n",
       " 'exhibition',\n",
       " 'of',\n",
       " '1903',\n",
       " '.',\n",
       " 'Simmel',\n",
       " 'was',\n",
       " 'originally',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'lecture',\n",
       " 'on',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'intellectual',\n",
       " '(',\n",
       " 'or',\n",
       " 'scholarly',\n",
       " ')',\n",
       " 'life',\n",
       " 'in',\n",
       " 'the',\n",
       " 'big',\n",
       " 'city',\n",
       " ',',\n",
       " 'but',\n",
       " 'he',\n",
       " 'reversed',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'the',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'city',\n",
       " 'on',\n",
       " 'the',\n",
       " 'mind',\n",
       " 'of',\n",
       " 'the',\n",
       " 'individual',\n",
       " '.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'result',\n",
       " ',',\n",
       " 'when',\n",
       " 'the',\n",
       " 'lectures',\n",
       " 'were',\n",
       " 'published',\n",
       " 'as',\n",
       " 'essays',\n",
       " 'in',\n",
       " 'a',\n",
       " 'book',\n",
       " ',',\n",
       " 'to',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'gap',\n",
       " ',',\n",
       " 'the',\n",
       " 'series',\n",
       " 'editor',\n",
       " 'had',\n",
       " 'to',\n",
       " 'supply',\n",
       " 'an',\n",
       " 'essay',\n",
       " 'on',\n",
       " 'the',\n",
       " 'original',\n",
       " 'topic',\n",
       " 'himself',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Simmel',\n",
       " 'compared',\n",
       " 'the',\n",
       " 'psychology',\n",
       " 'of',\n",
       " 'the',\n",
       " 'individual',\n",
       " 'in',\n",
       " 'rural',\n",
       " 'life',\n",
       " 'with',\n",
       " 'the',\n",
       " 'psychology',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'dweller',\n",
       " '.',\n",
       " 'His',\n",
       " 'investigation',\n",
       " 'determines',\n",
       " 'that',\n",
       " 'the',\n",
       " 'human',\n",
       " 'psychology',\n",
       " 'is',\n",
       " 'altered',\n",
       " 'by',\n",
       " 'the',\n",
       " 'metropolis',\n",
       " '.',\n",
       " 'The',\n",
       " 'individual',\n",
       " 'must',\n",
       " 'contend',\n",
       " 'with',\n",
       " 'such',\n",
       " 'change',\n",
       " 'in',\n",
       " 'a',\n",
       " 'metropolitan',\n",
       " 'environment',\n",
       " 'that',\n",
       " 'the',\n",
       " 'psychology',\n",
       " 'of',\n",
       " 'such',\n",
       " 'an',\n",
       " 'individual',\n",
       " 'erects',\n",
       " 'defences',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'itself',\n",
       " 'from',\n",
       " 'the',\n",
       " 'stimuli',\n",
       " 'of',\n",
       " 'the',\n",
       " 'metropolis',\n",
       " '.',\n",
       " 'As',\n",
       " 'such',\n",
       " ',',\n",
       " 'the',\n",
       " 'city',\n",
       " 'dwellerâ€',\n",
       " '™',\n",
       " 's',\n",
       " 'attitude',\n",
       " 'and',\n",
       " 'psychology',\n",
       " 'is',\n",
       " 'fundamentally',\n",
       " 'different',\n",
       " 'to',\n",
       " 'an',\n",
       " 'individual',\n",
       " 'that',\n",
       " 'inhabits',\n",
       " 'rural',\n",
       " 'life',\n",
       " '.',\n",
       " 'The',\n",
       " 'psychology',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'dweller',\n",
       " ',',\n",
       " 'therefore',\n",
       " ',',\n",
       " 'exhibits',\n",
       " 'what',\n",
       " 'Simmel',\n",
       " 'describes',\n",
       " 'as',\n",
       " 'adaptations',\n",
       " 'and',\n",
       " 'adjustments',\n",
       " 'which',\n",
       " 'ultimately',\n",
       " 'reflect',\n",
       " 'the',\n",
       " 'structures',\n",
       " 'of',\n",
       " 'the',\n",
       " 'metropolis',\n",
       " '.',\n",
       " 'Simmel',\n",
       " 'characterises',\n",
       " 'rural',\n",
       " 'life',\n",
       " 'as',\n",
       " 'a',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'meaningful',\n",
       " 'relationships',\n",
       " ',',\n",
       " 'established',\n",
       " 'over',\n",
       " 'time',\n",
       " '.',\n",
       " 'These',\n",
       " 'kinds',\n",
       " 'of',\n",
       " 'relationships',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'established',\n",
       " 'in',\n",
       " 'the',\n",
       " 'metropolis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'reasons',\n",
       " '(',\n",
       " 'e.g.',\n",
       " 'anonymity',\n",
       " ',',\n",
       " 'number',\n",
       " 'of',\n",
       " 'vendors',\n",
       " 'etc',\n",
       " '.',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " ',',\n",
       " 'the',\n",
       " 'city',\n",
       " 'dweller',\n",
       " 'can',\n",
       " 'only',\n",
       " 'establish',\n",
       " 'a',\n",
       " 'relationship',\n",
       " 'with',\n",
       " 'currency',\n",
       " 'â€',\n",
       " '“',\n",
       " 'money',\n",
       " 'and',\n",
       " 'exchange',\n",
       " 'becomes',\n",
       " 'the',\n",
       " 'medium',\n",
       " 'within',\n",
       " 'which',\n",
       " 'the',\n",
       " 'city',\n",
       " 'dweller',\n",
       " 'invests',\n",
       " 'their',\n",
       " 'trust',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'The',\n",
       " 'deepest',\n",
       " 'problems',\n",
       " 'of',\n",
       " 'modern',\n",
       " 'life',\n",
       " 'flow',\n",
       " 'from',\n",
       " 'the',\n",
       " 'attempt',\n",
       " 'of',\n",
       " 'the',\n",
       " 'individual',\n",
       " 'to',\n",
       " 'maintain',\n",
       " 'the',\n",
       " 'independence',\n",
       " 'and',\n",
       " 'individuality',\n",
       " 'of',\n",
       " 'his',\n",
       " 'existence',\n",
       " 'against',\n",
       " 'the',\n",
       " 'sovereign',\n",
       " 'powers',\n",
       " 'of',\n",
       " 'society',\n",
       " ',',\n",
       " 'against',\n",
       " 'the',\n",
       " 'weight',\n",
       " 'of',\n",
       " 'the',\n",
       " 'historical',\n",
       " 'heritage',\n",
       " 'and',\n",
       " 'the',\n",
       " 'external',\n",
       " 'culture',\n",
       " 'and',\n",
       " 'technique',\n",
       " 'of',\n",
       " 'life',\n",
       " '.',\n",
       " 'The',\n",
       " 'antagonism',\n",
       " 'represents',\n",
       " 'the',\n",
       " 'most',\n",
       " 'modern',\n",
       " 'form',\n",
       " 'of',\n",
       " 'the',\n",
       " 'conflict',\n",
       " 'which',\n",
       " 'primitive',\n",
       " 'man',\n",
       " 'must',\n",
       " 'carry',\n",
       " 'on',\n",
       " 'with',\n",
       " 'nature',\n",
       " 'for',\n",
       " 'his',\n",
       " 'own',\n",
       " 'bodily',\n",
       " 'existence',\n",
       " '.',\n",
       " 'The',\n",
       " 'eighteenth',\n",
       " 'century',\n",
       " 'may',\n",
       " 'have',\n",
       " 'called',\n",
       " 'for',\n",
       " 'liberation',\n",
       " 'from',\n",
       " 'all',\n",
       " 'the',\n",
       " 'ties',\n",
       " 'which',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'historically',\n",
       " 'in',\n",
       " 'politics',\n",
       " ',',\n",
       " 'in',\n",
       " 'religion',\n",
       " ',',\n",
       " 'in',\n",
       " 'morality',\n",
       " 'and',\n",
       " 'in',\n",
       " 'economics',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'the',\n",
       " 'original',\n",
       " 'natural',\n",
       " 'virtue',\n",
       " 'of',\n",
       " 'man',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'equal',\n",
       " 'in',\n",
       " 'everyone',\n",
       " ',',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'without',\n",
       " 'inhibition',\n",
       " ';',\n",
       " 'the',\n",
       " 'nineteenth',\n",
       " 'century',\n",
       " 'may',\n",
       " 'have',\n",
       " 'sought',\n",
       " 'to',\n",
       " 'promote',\n",
       " ',',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'freedom',\n",
       " ',',\n",
       " 'his',\n",
       " 'individuality',\n",
       " '(',\n",
       " 'which',\n",
       " 'is',\n",
       " 'connected',\n",
       " 'with',\n",
       " 'the',\n",
       " 'division',\n",
       " 'of',\n",
       " 'labor',\n",
       " ')',\n",
       " 'and',\n",
       " 'his',\n",
       " 'achievements',\n",
       " 'which',\n",
       " 'make',\n",
       " 'him',\n",
       " 'unique',\n",
       " 'and',\n",
       " 'indispensable',\n",
       " 'but',\n",
       " 'which',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'make',\n",
       " 'him',\n",
       " 'so',\n",
       " 'much',\n",
       " 'the',\n",
       " 'more',\n",
       " 'dependent',\n",
       " 'on',\n",
       " 'the',\n",
       " 'complementary',\n",
       " 'activity',\n",
       " 'of',\n",
       " 'others',\n",
       " ';',\n",
       " 'Nietzsche',\n",
       " 'may',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'relentless',\n",
       " 'struggle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'individual',\n",
       " 'as',\n",
       " 'the',\n",
       " 'prerequisite',\n",
       " 'for',\n",
       " 'his',\n",
       " 'full',\n",
       " 'development',\n",
       " ',',\n",
       " 'while',\n",
       " 'socialism',\n",
       " 'found',\n",
       " 'the',\n",
       " 'same',\n",
       " 'thing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'suppression',\n",
       " 'of',\n",
       " 'all',\n",
       " 'competition',\n",
       " 'â€',\n",
       " '“',\n",
       " 'but',\n",
       " 'in',\n",
       " 'each',\n",
       " 'of',\n",
       " 'these',\n",
       " 'the',\n",
       " 'same',\n",
       " 'fundamental',\n",
       " 'motive',\n",
       " 'was',\n",
       " 'at',\n",
       " 'work',\n",
       " ',',\n",
       " 'namely',\n",
       " 'the',\n",
       " 'resistance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'individual',\n",
       " 'to',\n",
       " 'being',\n",
       " 'levelled',\n",
       " ',',\n",
       " 'swallowed',\n",
       " 'up',\n",
       " 'in',\n",
       " 'the',\n",
       " 'social',\n",
       " '-',\n",
       " 'technological',\n",
       " 'mechanism',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Simmel',\n",
       " 'seeks',\n",
       " 'to',\n",
       " 'explain',\n",
       " 'human',\n",
       " 'nature',\n",
       " 'and',\n",
       " 'how',\n",
       " 'it',\n",
       " 'plays',\n",
       " 'a',\n",
       " 'part',\n",
       " 'in',\n",
       " 'society',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Man',\n",
       " \"'s\",\n",
       " 'nature',\n",
       " ',',\n",
       " 'originally',\n",
       " 'good',\n",
       " 'and',\n",
       " 'common',\n",
       " 'to',\n",
       " 'all',\n",
       " ',',\n",
       " 'should',\n",
       " 'develop',\n",
       " 'unhampered',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'more',\n",
       " 'liberty',\n",
       " ',',\n",
       " 'the',\n",
       " 'nineteenth',\n",
       " 'century',\n",
       " 'demanded',\n",
       " 'the',\n",
       " 'functional',\n",
       " 'specialization',\n",
       " 'of',\n",
       " 'man',\n",
       " 'and',\n",
       " 'his',\n",
       " 'work',\n",
       " ';',\n",
       " 'this',\n",
       " 'specialization',\n",
       " 'makes',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'incomparable',\n",
       " 'to',\n",
       " 'another',\n",
       " ',',\n",
       " 'and',\n",
       " 'each',\n",
       " 'of',\n",
       " 'them',\n",
       " 'indispensable',\n",
       " 'to',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'possible',\n",
       " 'extent',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmel_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of tokens are also in a mess, but we will clean it later in Task 4. So let's keep this list for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XclveJg1L1jT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 19\n",
      "Number of tokens: 652\n",
      "Number of unique tokens (or types): 289\n",
      "Average sentence length: 189.579\n",
      "Average token length: 4.669\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(simmel_sents_list)\n",
    "num_tokens = len(simmel_tokens_list)\n",
    "num_unique_tokens = len(set(simmel_tokens_list))\n",
    "avg_sentence_len = round(sum(len(elem) for elem in simmel_sents_list) / num_sentences, 3)\n",
    "avg_token_len = round(sum(len(elem) for elem in simmel_tokens_list) / num_tokens, 3)\n",
    "\n",
    "print(\"Number of sentences:\", num_sentences)\n",
    "print(\"Number of tokens:\", num_tokens)\n",
    "print(\"Number of unique tokens (or types):\", num_unique_tokens)\n",
    "print(\"Average sentence length:\", avg_sentence_len)\n",
    "print(\"Average token length:\", avg_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "j8zafNOuN4PC"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_lengths = [len(elem) for elem in simmel_sents_list]\n",
    "\n",
    "# draw the histogram \n",
    "n, bins, patches = plt.hist(sentence_lengths, bins = 15, facecolor='red', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nZbqdQxe-5ks"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_lengths = [len(elem) for elem in simmel_tokens_list]\n",
    "\n",
    "# draw the histogram\n",
    "n, bins, patches = plt.hist(token_lengths, bins = 15, facecolor='red', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlrAicfJOC6S"
   },
   "source": [
    "## Task 3: Bype pair encoding (BPE) tokenization (1 point) \n",
    "\n",
    "### Task 3.1 (0.25 points)\n",
    "\n",
    "Byte pair encoding (BPE) [link text](https://en.wikipedia.org/wiki/Byte_pair_encoding) is a simple algorithm of data compression. It looks for the most frequent pair of bytes in the data and replaces it with a new byte which is not seen in the data.\n",
    "\n",
    "Recently, this idea became used in the [tokenization](https://www.aclweb.org/anthology/P16-1162.pdf). Let's say that we want to train a network that captures the meaning of words. We can have in out data the following words: low, lower, lowest. If we tokenize the text in a simple way by splitting the words as a whole, the model will probably learn the relation between low, lower, lowest. Now, imagine that we get some new text that the model didn't see during training and it has the words small, smaller, smallest and in the training data we had only the word small. Since the model didn't see smaller and smallest during the training, it will most likely fail to capture the relation.\n",
    "\n",
    "One of the ways to solve this is BPE tokenization. It learns the most frequent sequences and can split an unknown word into **subwords**. In our case, it can split smaller into ['small', 'er'] since we had small in the training data and probably many other words ending with -er. Now. instead of one unknown word, the model have two known subwords from which it can take the information.\n",
    "\n",
    "The code below builds the subwords from the text data. For the purpose of time saving, we set the number of merges to 1000.\n",
    "\n",
    "**Study the code below and answer the questions after it.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3jCjzoj1OPSp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['<', '!', 'D', 'O', 'C', 'T', 'Y', 'P', 'E', '</w>', 'h', 't', 'm', 'l', '>', 'c', 'a', 's', '=', '\"', 'i', 'e', 'n', '-', 'o', 'j', 'g', 'd', 'r', 'U', 'F', '8', '/', 'M', 'p', 'L', 'f', 'W', 'k', 'u', '.', 'N', ';', 'R', '{', 'w', 'B', ':', '1', ',', 'S', 'b', '[', ']', 'y', 'J', 'A', 'v', 'q', 'I', '@', 'Z', 'Q', '0', '_', '9', '7', '2', '5', '6', 'G', '*', '3', 'x', 'V', '}', '(', '|', ')', 'z', '$', '+', '\\\\', '?', '&', '%', '4', 'H', '#', 'K', 'É', 'ü', 'ö', 'ß', 'ä', \"'\", '’', '–', '—', '^', '‐', 'Б', 'о', 'л', 'ь', 'ш', 'и', 'е', 'г', 'р', 'д', 'а', 'у', 'х', 'в', 'н', 'я', 'ж', 'з', 'Р', 'с', 'к', 'й', '®'])\n",
      "Number of tokens: 124\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8217213b920b42e7add5d1efd92f2020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All tokens: dict_keys(['<!', 'D', 'O', 'C', 'T', 'Y', 'P', 'E', '</w>', 'ht', 'm', 'l', '></w>', '<', 'l</w>', 'class=\"', 'c', 'li', 'ent', '-', 'no', 'j', 's\"</w>', 'lang=\"', 'en', '\"</w>', 'di', 'r', '=\"', 'ltr', '\"></w>', 'head', '<meta</w>', 'ch', 'ar', 'se', 't=\"', 'U', 'F', '8', '\"/></w>', 'title', '>', 'The</w>', 'Metropolis</w>', 'and</w>', 'Mental</w>', 'Life', 'Wikipedia', '</', 'script', 'do', 'u', 'ment', '.', 'le', 'class', 'Name', 's', '\"', ';', 'RL', 'N', '=', '{\"', 'wg', 'B', 're', 'a', 'k', 'ra', 'me', '\":!1,\"wg', 'Se', 'p', 'at', 'or', 'an', 'form', 'able', '\":[', '\",\"', '\"]', ',\"wg', 'ig', 'it', 'De', 'fa', 'ult', 'ate', '\":\"', 'd', 'y', '\",\"wg', 'M', 'on', 'th', 'J', 'ary', 'e', 'br', 'arch', 'A', 'ri', 'Ma', 'un', 'ul', 'g', 'st', 'te', 'mber', 'to', 'ber', 'No', 've', 'December', 'Re', 'qu', 'Id', 'W', '@', 'w', 'I', 'Z', 'Q', 'CS', 'ce', 'anon', 'ic', 'al', 'space', 'Speci', 'Page', 'umber', '\":', '0,\"wg', 'The_Metropolis_and_Mental_Life', 'i', 'tle', 'ur', 'vi', 'si', '997292977', 'Article', '25', '27', '90', '6', '\":!0,\"wg', 'R', 'edi', 'rect', 'ction', 'view', 'ser', 'n', 'G', 'ro', 'up', '*', 'Categ', 'ie', 'Articles</w>', 'with</w>', 'sh', 't</w>', 'description', 'S', 'hor', 'che', 's</w>', 'Wiki', 'data', 'article', 'unsource', 'd</w>', 'statement', 'from</w>', '2020', '190', '3</w>', 'essa', 'non', 'fi', 'books', 'ssa', 'y</w>', 'German', 'Sociology</w>', 'Work', 'about</w>', 'ci', 'tie', ',</w>', 'soci', 'ety', 'Content', 'L', 'uage', 'Mo', 'de', 'wiki', 'text', 'v', 'ant', 'b', 'ab', 'ly', 'Edit', ']', 'edia', 'Vi', 'ew', 'er', 'ck', 'na', 'led', 'op', 'Refe', 'rence', 'Con', 'fli', 'ct', 'ith', 'Nav', 'adge', 't', 'f', 'o', 'ol', 'ti', 'ps', 'V', 'is', '\":{\"', 'page', 'Co', 'ariant', 'all', 'back', '\"}', 'sp', 'lay', 'base', 'scription', 'search', '\":!', '0', ',\"', 'ne', 'by', 'list', 'line', '\":!1', '}', 'ma', 'mp', 'ver', 'amp', 'nt', 'ut', 'ng', 'pedia', 'bi', 'in', 'Sub', 'mit', 'utton', 'el', 'ub', 'osi', 'tion', 'interlanguage', '12', '13', '1', '88', 'ext.', 'lo', 'ss', 's.', 'user', 'styles', '\":\"ready', 'site', '\",\"ext.', 'ading', 'cite', 'ski', 'vector', 'style', 'acy', 'su', 'top', 'arge', 't.', 'media', '.c', '[', 'x', 'h', 's\",\"ext.', 'sc', 'rib', 'log', 'mediawiki', '.p', 'age', 'ready', 'toc', 'ex', 'tr', 'bar', 'ref', 'cent', 'la', 'olog', 'mm', 'bo', 'au', 'Lo', 'ad', 'vent', 'gin', 'navigation', 'im', 'co', 'ac', 'link', 'inter', 'ace', 'ge', 'script></w>', '(', 'dow', '|', ')', 'function', '{', 'mw', 'load', 'z', '\",', '$', ',', 'Qu', 'mo', 'du', '/', 'min', 'ok', '+', '\\\\', 'rf', ';</w>', '<link</w>', 'rel=\"', 'he', 'et', 'href=\"', '/w/', '.php?', 'lang', '&amp;', '%', '7', 'scrip', 'src=\"', '></', 'name=\"', 'source', 'St', 'yle', 'content', '1.', '3', '.0', '2', 'origin', '-c', 'rop', 'ty', ':', 'image', 'https://', 'upload.wikimedia.org/wikipedia/commons/', 'thumb/', '4/', 'Social_Network_Diagram_%28segment%29', '.svg/', '00', 'px-', '.svg.png', 'con', 'href=\"//', 'upload.wikimedia.org', 'ter', 'width', ':</w>', '20', 'px', ')\"</w>', '.wikipedia.org', '/wiki/', 'type=\"', 'app', 'cation', 'title=\"Edit</w>', 'this</w>', 'href=\"/w/index.php?title=The_Metropolis_and_Mental_Life&amp;action=edit', 'edit', '-t', 'ou', '/static/', '/wikipedia', '.png', 'ut</w>', '_', '.php', 'title=\"', 'edia</w>', 'en.wikipedia.org', 'ap', 'action=', 'cense', 'ative', 'common', '.org/', 's/', 'href=\"https://', '-p', '.wikimedia.org', 'met', '/></w>', 'bod', 'mw-', 'hid', 's-', '0</w>', 'sub', 'able</w>', 'tp', 'n-', 'action', '\"><div</w>', 'id=\"', 'mw-p', 'class=\"noprin', '\"></div></w>', '<div</w>', 'id=\"mw-', 'class=\"mw-', 'body', 'role=\"', '<a</w>', '></a', '\"><', '!', '--</w>', 'ce</w>', '--', '></div></w>', 'content\"></w>', '</div></w>', '<h', '1</w>', 'He', 'i>', '\">', 'm</w>', 'the</w>', 'e</w>', 'nav', '-link', 'href=\"#', 'to</w>', '</a', 'In', 'content-', 'text\"</w>', 'mw-parser-outp', 'le</w>', 'prin', 'style=\"', 'display', ':none', '99', 'book', 'by</w>', 'Georg</w>', 'Simmel', '><', '</p', 'tic', 'box', 'now', 'right', 'em;', 'margin', 'em</w>', 'background:', '#', '9', 'border', ':1', 'px</w>', 'solid</w>', '#aaa', 'padding:', '0.2em;', 'border-', '0.4', ';text-align:', 'center', 'line-', 'height', '4', 'font-size', 'tr><td</w>', 'padding-top:', '0.4em;', 'em', 'of</w>', 'a</w>', 'tr><', 'style=\"padding:', '0.2em</w>', '0;', 'font-size:1', '5', '%;', '\"><a</w>', 'href=\"/wiki/Sociology', 'Sociology', '</a></', 'tr><tr><td</w>', 'href=\"/wiki/', 'Social_Network_Diagram_', 'segment', '.svg', 'g</w>', 'Social</w>', 'Network', 'iagra', '//upload.wikimedia.org/wikipedia/commons/thumb/', 'ding', 'width=\"', 'height=\"', '97', 'thumb', 'src', '1.5', '2x', 'data-', 'file', '></td', '0.3', 'font-', 'we', 'ight', 'padding-', '<ul><li><a</w>', 'Hist', 'ory', '_of_', 'sociology\"</w>', 'sociology\">', '</a></li></w>', '<li', '><a</w>', 'lin', 'li></w>', '<li><a</w>', 'dex', 'sociology', 's\">', '</a></li></ul', '0.1em</w>', 'class=\"Nav', 'me</w>', 'collap', 'd\"</w>', ':none;', 'style=\"font-size:105%;', 'transparent', ';text-align:left', 'dd', 'Soci', 'al_', 'theory', 'ical</w>', 'The', 'div', '><div</w>', 'text-align:', 'the', 'ali', 'sm', 'ural</w>', 'mb', 'olic', 'interaction', 'ism', 'al</w>', 'tivi', 'href=\"/wiki/Social_', 'change', 'title=\"Social</w>', 'stru', 'ent</w>', '</a></li></ul></div', '></div', 'research', 'alit', 'Comp', 'hist', 'ation', 'al_sociology\"</w>', 'al</a></li></w>', 'graphy', 'graph', 'analy', 'ation</w>', 's</a></li></w>', '_(', ')\">', 'etwork', 'man', 's_', 'of_', 'class=\"mw-redirect\"</w>', 'ology', 'href=\"/wiki/Sociology_of_', 'ure', 'title=\"Sociology</w>', 'title=\"D', 'velop', 'En', 'mi', 'ist', 'der', 'and', 'imm', 'K', 'w\"</w>', 'Li', 'rg', 'anization', 'oli', 'sci', 'entifi', 'psychology', 'psychology</w>', 'in</w>', 'ati', 'Te', '\">T', 'an</w>', 'sociolog', 'st</w>', '%8', '_D', 'É', 'H', 't_', 'ed', 'ch</w>', 'us', 'title=\"A', 'te</w>', 'Ge', '_M', 'Georg', '.</w>', 'nd', 'ca', 'of', 'ing</w>', '%B', 'ü', 'ble', 'ö', 'm_', 'ha', 'href=\"/wiki/Category:', 'title=\"Category:', 'ourn', 'oci', 'count', 'Social_', 'sciences', '16', 'class=\"no', '&#', 'al:', 'portal', '15', 'padding-top', '0.', 'ded', 'cate', 'Template', 's:', '95', '.mw-parser-output</w>', '.navbar', '}.mw-parser-output</w>', 'left', 'lock', 'whi', 'te-', 'herit', 'bracket', 'fo', '-m', 'text-', 't-', 'info', '.nav', '><li</w>', 'Template:', 'template', '></a></', 'alk', '/w/index.php?title=', '></ul', '></t', 'language', '</a>', '<i>', 'ß', 'ä', 'as</w>', 'is</w>', 'class=\"toc', 'role=\"navigation\"</w>', 'aria-labelledby=\"', 'checkbox', 'gg', '2</w>', '</h', '><span</w>', 'span', 'label', 'for', '></span', '<ul', '<li</w>', 'section-', '\"><span</w>', '</span></w>', '<span</w>', '</span', 'ren', 'ces', 'ther', 'class=\"mw-edit', 'section', 'class=\"mw-editsection-bracket\">', \"'\", 'wid', 'read', 'work', 'Metropoli', 'id', 'on</w>', 'cit', 'fe', 'exp', 'ari', 'scien', 'den', 'hi', 'ke', 'role</w>', 'order', 'ze', 'individu', 'sup', 'href=\"/wiki/Wikipedia:', 'Cit', 'nee', 'Wikipedia:', 'his</w>', 'cla', 'reference', 'sour', 'citation</w>', '93', 'are', 'individual</w>', 'wel', 'etropoli', 'conten', '’', 'und', 'which</w>', 'se</w>', 'ki', 'not</w>', 'son', '–', 'with', 'cite_', '44', 'quote', 'quo', 'pe', 'n</w>', 'dep', 'ow', 'it</w>', 'sion</w>', 'lab', 'disp', '—', 'cated', 'tent', '</a></li></ul></w>', 'type', 'refe', '^', 'q', 'lock-', ',.mw-parser-output</w>', '.cs1-', 'gra', '//upload.wikimedia.org/wikipedia/commons/', 'rig', 'aa', 'col', '-logo', '.cs', '0.2', '05', 'ani', 'identifier', '0.1', '%2F', '43', '10', 'href=\"/wiki/Special:', 'our', '1-', 'Special:', '39', '&amp;rf', '%3A', '&amp;rft.', 'title=', '%2', 'Ment', '4.', 'ork', 'background', 'td</w>', 'tex', 'late', '.wiki', '.org', '><li', 'gn', 'ww', '/st', 'ue', 'name', '%3', '<!--</w>', 'por', '92', '‐', 'cess', '0000', 'size', 'arser', 'rip', '49', 'ase</w>', 'enti', '--></w>', '%</w>', '//', '?', 'none', 'footer', 'The_Metropolis_and_Mental_Life&amp;', 'Category', '</a></li><li><a</w>', 'abou', 'ateg', 'fro', 'Na', 'vig', 'enu', 'Please</w>', 'do</w>', 'use</w>', 'attribute</w>', 'CSS</w>', 'selector,</w>', 'deprecated.</w>', '<nav</w>', 'id=\"p-', 'class=\"mw-portlet</w>', 'mw-portlet-', 'vector-menu', 'aria-labelledby=\"p-', '-label\"</w>', '<h3</w>', '-label\"></w>', '<span>', '</h3></w>', 'class=\"vector-menu-content\"></w>', '<ul</w>', 'class=\"vector-menu-content-list\"><li</w>', ']\"</w>', 'accesskey=\"', '</a></li><li</w>', 'trib', 'tribu', 'href=\"/w/index.php?title=', 'cou', '</nav></w>', 'lef', 'vector-menu</w>', 'vector-menu-', 'sele', 'page</w>', 'ortle', 'class=\"vector-menu-', 'class=\"vector-menu-content-list\"', 'href=\"/w/index.php?title=The_Metropolis_and_Mental_Life&amp;action=', 'Search', '/w/index', 'value', 'utt', 'logo', 'navigation\"</w>', 'navig', 'port', 'id=\"n-', 'ctor', 'oundation', 'ange', 'cen', 'upload', 'id=\"t-', 'href=\"/w/index.php?title=The_Metropolis_and_Mental_Life&amp;', 'erman', 'entifier', 'dat', 'rin', 'href', 'pedia.org', 'ru', '%D0', '%D0%B', '%D', 'Б', 'о', 'л', 'ь', 'ш', 'и', 'е', 'г', 'р', 'д', 'а', 'у', 'х', 'в', 'н', 'я', 'ж', 'з', 'Р', 'с', 'к', 'й', 'Edit</w>', 'id=\"footer', 'mmon', '®', 'k</w>', '.wikimedia', 'org', 'id=\"footer-pla', 'id=\"footer-places', 'https:', '/wiki', 'id=\"footer-p', 'limit', '\":{\"value\":', ',\"limit\":', '},\"', 'access', '\",\"</w>', '\":{\"value', 'tran', 'conte', '\\\\/', 'http', '/w', 'atic', 'ack'])\n",
      "Number of tokens: 1006\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(filename):\n",
    "    \"\"\"Gets the text from a file and splits it with spaces.\"\"\"\n",
    "    \n",
    "    vocab = Counter()\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Computes the frequencies for each pair of characters in the vocab.\"\"\"\n",
    "\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, in_vocab):\n",
    "    \"\"\"Merges the most frequent pair.\n",
    "\n",
    "    Arguments:\n",
    "    pair -- the most frequent word pair (tuple(str, str))\n",
    "    in_vocab -- vocabulary with frequencies (dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    out_vocab = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in in_vocab:\n",
    "        out_word = p.sub(''.join(pair), word)\n",
    "        out_vocab[out_word] = in_vocab[word]\n",
    "    return out_vocab\n",
    "\n",
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = Counter()\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "\n",
    "vocab = get_vocab(data_path)\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')\n",
    "\n",
    "num_merges = 1000\n",
    "for i in tqdm(range(num_merges)):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8EJzkCfOTGv"
   },
   "source": [
    "Answer the following questions: \n",
    "\n",
    "**Study the subwords from your data. Do you see any subwords that make sense from the linguistic point of view? (e.g suffixes, prefixes, common roots etc.). Provide examples.**\n",
    "\n",
    "Answer: I couldn't find many sensible tokens but if we consider the fact that my raw text is an html file with lots of repetitive tags, encountering a few number of morphemes is not a big surprise. The subwords I was able to detect are '-able', '-s' and 'un-', with all of them being an affix. I couldn't find any stems that can be potentially combined together with an affix, though there were meaningful tokes such as 'space', 'statement' and 'search'.\n",
    "\n",
    "**What will happen if you increase the number of merges?**\n",
    "\n",
    "Answer: The vocabulary output of BPE tokenization will get bigger. This means a potential increase in the number of morphemes, thus making our vocabulary more generalizable compared to the case with small number of merges. \n",
    "\n",
    "(Yet in my case, in order to really make use of the increased number of merges, we should clear the text from html tags first, otherwise BPE tokenization will capture common html tags like 'id=footer-places', 'id=footer-pla', 'footer' etc, which are useless for text analysis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-eeRrZIOqUx"
   },
   "source": [
    "### Task 3.2 (0.75 points) \n",
    "Now, you are going to implement the function that splits the unknown word into subwords using the vocab that we built above.\n",
    "\n",
    "One way to do it is the following:\n",
    "\n",
    "1. Sort our vocab by the length in the descending order.\n",
    "2. Find the boundaries of the \"window\" that is going to search if a candidate word has a corresponding subword in the vocab. In the beginning, the starting index is 0, since we start to scan the word from the first characher. The end index is the length of the longest subword in the vocab or the length of the word if it is smaller.\n",
    "3. In a while loop, start looking at the possible subwords. If the subword you are looking at is in the vocab, append it to the result. Now, your new starting index is your previous end index. Your new end index is your new start index plus the length of the longest subword in the vocab or the length of the word if it is smaller than the resulting sum. If the subword is not in the vocab, we reduce the end index by one thus narrowing our search window. Finally, is the length of our window is equal to one, we put an unknown subword in the result and update our window as above.\n",
    "4. End the loop when we reach the end of the word.\n",
    "\n",
    "After you finish with the function, test the tokenizer on a very common word and on a very unusual word (you can even try to invent a word yourself).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HVDRAD-ZO_pa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: metropolis</w>...\n",
      "['m', 'etropoli', 's</w>']\n",
      "Tokenizing word: dragonborn</w>...\n",
      "['d', 'r', 'a', 'g', 'o', 'n', 'b', 'o', 'r', 'n', '<', '/', 'w', '>']\n"
     ]
    }
   ],
   "source": [
    "# Sorting the subwords by the length in the descending order\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the word into subword using learned BPE vocab\n",
    "    \n",
    "    Arguments:\n",
    "    string -- a word to tokenize. Must end with </w>\n",
    "    sorted_tokens -- sorted vocab by frequency in descending order\n",
    "    unknown_token -- a token to replace the words not found in the vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    # We are going to store our subwords here\n",
    "    string_tokens = []\n",
    "    \n",
    "    # Find the maximum length of the ngram in vocab\n",
    "    ngram_max_len = len(sorted_tokens_tuple[0][0])\n",
    "    # End index is the maximum length of the ngram or the length of the string if it's smaller\n",
    "    end_idx = (ngram_max_len if len(string) > ngram_max_len else len(string))\n",
    "    # Starting index is 0 in the beginning\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(string):\n",
    "        subword = string[start_idx:end_idx]\n",
    "        if subword in sorted_tokens:\n",
    "            string_tokens.append(subword)\n",
    "            start_idx = end_idx\n",
    "            end_idx = start_idx + (ngram_max_len if len(subword) > ngram_max_len else len(subword))\n",
    "        elif len(subword) == 1:\n",
    "            string_tokens.append(unknown_token)\n",
    "            start_idx = end_idx\n",
    "            end_idx = start_idx + (ngram_max_len if len(subword) > ngram_max_len else len(subword))\n",
    "        else:\n",
    "            end_idx -= 1\n",
    "            \n",
    "    return string_tokens\n",
    "\n",
    "# The word should end with \"</w>\". For example, \"cat</w>\".\n",
    "word_known = 'metropolis</w>'\n",
    "word_unknown = 'dragonborn</w>'\n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_known))\n",
    "if word_known in vocab_tokenization:\n",
    "    print(vocab_tokenization[word_known])\n",
    "else:\n",
    "    print(tokenize_word(string=word_known, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "    \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_unknown))\n",
    "if word_unknown in vocab_tokenization:\n",
    "    print(vocab_tokenization[word_unknown])\n",
    "else:\n",
    "    print(tokenize_word(string=word_unknown, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efwxuk66PFu3"
   },
   "source": [
    "## Task 4: Lemmatization and normalization (1 point) \n",
    "\n",
    "### Task 4.1 (0.5 points) \n",
    "\n",
    "Using either NTLK or Spacy, lemmatize your data. Make a copy of your data but this time transform all the tokens and lemmas into the lowercase.\n",
    "\n",
    "Provide the following statistics:\n",
    "\n",
    "* Number of unique lemmas (original case)\n",
    "* Number of unique lemmas (lower case)\n",
    "* Number of unique tokens (original case)\n",
    "* Number of unique tokens (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmel_lemmas_list = [token.lemma_ for token in simmeldoc if not token.is_stop and not token.is_punct and not token.like_num] \n",
    "simmel_lemmas_list = [lemma for lemma in simmel_lemmas_list if lemma!='\\n\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simmel_lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_words = ['die','GroÃŸstÃ¤dte','und','das','Geistesleben']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmel_lemmas_list = [word for word in simmel_lemmas_list if not word in german_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simmel_lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmel_tokens_list_lower = [token.lower() for token in simmel_tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simmel_tokens_list_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8V_7fYSsPQEe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (original case): 191\n",
      "Number of unique lemmas (lower case): 187\n",
      "Number of unique tokens (original case): 289\n",
      "Number of unique tokens (lower case): 280\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize your data\n",
    "# Lemmatizing took several steps and manual checks above, so I will just copy the results here\n",
    "simmel_lemmas_list = simmel_lemmas_list\n",
    "\n",
    "\n",
    "# Make a copy of your tokens but in lowercase\n",
    "simmel_tokens_list_lower = [token.lower() for token in simmel_tokens_list]\n",
    "simmel_lemmas_list_lower = [lemma.lower() for lemma in simmel_lemmas_list]\n",
    "\n",
    "\n",
    "# Count statistics (no need to calculate the number of unique tokens in original case since we did it in Task 2)\n",
    "num_unique_lemmas = len(set(simmel_lemmas_list))\n",
    "num_unique_lemmas_lower = len(set(simmel_lemmas_list_lower))\n",
    "num_unique_tokens_lower = len(set(simmel_tokens_list_lower))\n",
    "\n",
    "# Print out the numbers\n",
    "print(\"Number of unique lemmas (original case):\", num_unique_lemmas)\n",
    "print(\"Number of unique lemmas (lower case):\", num_unique_lemmas_lower)\n",
    "print(\"Number of unique tokens (original case):\", num_unique_tokens)\n",
    "print(\"Number of unique tokens (lower case):\", num_unique_tokens_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjFUP4SqPUup"
   },
   "source": [
    "### Task 4.2 (0.5 points)\n",
    "\n",
    "Look at the numbers you got. \n",
    "\n",
    "**Imagine that you want to use your data to train a network that captures the meaning of the words. Do you want to use tokens or lemmas? Original or lowercase? Explain your choices.**\n",
    "\n",
    "Answer: I would use \"lowercase lemmas\". We are after the meaning of words, we should get rid of the difference between \"Small\" and 'small', for example, since they mean the same thing. When it comes to tokens vs lemmas, I'd say that in most cases lemmas would be a better option. Even though lemmatization will distort the meaning of words a little bit(smaller, smallest, small > small / done, did, do, doing > do. These tokens are quite close in meaning, but not actually the same, so we lose a bit of meaning during lemmatization), we can go for it since it will reduce the feature space considerably. Maybe in some rare occasions we should not go for lemmatization, but this depends on the particular text classification problem.\n",
    "\n",
    "**Imagine that you want to use your data to train a system that detects named entities, i.e. names of people, places, companies etc. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\n",
    "\n",
    "Answer: I would use \"original tokens\". This is because the named entities are referring to one specific object physically or legally existing in the world(or universe, if we are dealing with astronomical text data). Consider a named entity like \"Bigger and Bigger Capitalist Co.\". If we lemmatize tokens, we will probably end up with something like \"Big Capital\", and if we lowercase them; the entity will reduce to two commonly used words, \"big capital\". Chances are high that the text will include these normalized words in another section of it, thus making it impossible for us to detect the original named entity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wp82pLGsPsBa"
   },
   "source": [
    "## Task 5: Different Pipelines (0.5 points) \n",
    "\n",
    "In the next tasks you need to process your data from task 1 with two different pipelines. Use Stanza and spiCy for that. \n",
    "\n",
    "**What components do the pipelines have?**\n",
    "\n",
    "Answer: Stanza has 6 components : TokenizeProcessor, MWTProcessor(for expanding multi-word tokens), LemmaProcessor, POSProcessor, DepparseProcessor, NERProcessor and SentimentProcessor(for detecting sentiment in text).\n",
    "\n",
    "spaCy has 5 builtin components : Tokenizer, Tagger(for pos tagging), Parser(for finding dependency relations), NER(for finding named entitites) and TextCat(for assigning document labels). One can also add custom components to spaCy.\n",
    "\n",
    "**What languages do the pipelines support?**\n",
    "\n",
    "Answer: Stanza supports 66 languages for tokenization, MWT expansion, lemmatization, POS and morphological features tagging and dependency parsing. It also supports 13 languages for NER. More info [here.](https://stanfordnlp.github.io/stanza/available_models.html) \n",
    "\n",
    "spaCy supports 18 languages, although with varying degrees. For example there is no pos tagger for Russian in spaCy, yet it supports one for English. More info [here.](https://spacy.io/usage/models#languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htTy9qWSQBe0"
   },
   "source": [
    "## Task 6: Process your text (2 points) \n",
    "\n",
    "### Task 6.1 (1.5 point) \n",
    "\n",
    "Process the text data from the first task with two different pipelines. Use Stanza and spiCy for that. \n",
    "\n",
    "Select one sentence from the processed document and print out all the results (tokens, pos-tags, lemmas, depparse, etc.) from both pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are for drawing the dependency relations. Spacy can do it already, but not as accurate as stanza, \n",
    "# so i will use the deprels given by stanza and draw them with displacy.\n",
    "\n",
    "#pip install spacy-stanza\n",
    "from spacy_stanza import StanzaLanguage\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "R2vs3mXAQOk5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canberk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FORM</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>UPOS</th>\n",
       "      <th>XPOS</th>\n",
       "      <th>FEATS</th>\n",
       "      <th>HEAD</th>\n",
       "      <th>DEPREL</th>\n",
       "      <th>DEPS</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>Definite=Def|PronType=Art</td>\n",
       "      <td>2</td>\n",
       "      <td>det</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=347|end_char=350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>series</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>4</td>\n",
       "      <td>nsubj:pass</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=351|end_char=357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin</td>\n",
       "      <td>4</td>\n",
       "      <td>aux:pass</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=358|end_char=361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>conducted</td>\n",
       "      <td>conduct</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>Tense=Past|VerbForm=Part|Voice=Pass</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=362|end_char=371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>alongside</td>\n",
       "      <td>alongside</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>case</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=372|end_char=381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>Definite=Def|PronType=Art</td>\n",
       "      <td>9</td>\n",
       "      <td>det</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=382|end_char=385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Dresden</td>\n",
       "      <td>Dresden</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>8</td>\n",
       "      <td>compound</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=386|end_char=393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>cities</td>\n",
       "      <td>city</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>Number=Plur</td>\n",
       "      <td>9</td>\n",
       "      <td>compound</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=394|end_char=400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>4</td>\n",
       "      <td>obl</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=401|end_char=411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>case</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=412|end_char=414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1903</td>\n",
       "      <td>1903</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>NumType=Card</td>\n",
       "      <td>9</td>\n",
       "      <td>nmod</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=415|end_char=419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>punct</td>\n",
       "      <td>None</td>\n",
       "      <td>start_char=419|end_char=420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID        FORM       LEMMA   UPOS XPOS                                                  FEATS  HEAD      DEPREL  DEPS  \\\n",
       "0    1         The         the    DET   DT                              Definite=Def|PronType=Art     2         det  None   \n",
       "1    2      series      series   NOUN   NN                                            Number=Sing     4  nsubj:pass  None   \n",
       "2    3         was          be    AUX  VBD  Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin     4    aux:pass  None   \n",
       "3    4   conducted     conduct   VERB  VBN                    Tense=Past|VerbForm=Part|Voice=Pass     0        root  None   \n",
       "4    5   alongside   alongside    ADP   IN                                                   None     9        case  None   \n",
       "5    6         the         the    DET   DT                              Definite=Def|PronType=Art     9         det  None   \n",
       "6    7     Dresden     Dresden  PROPN  NNP                                            Number=Sing     8    compound  None   \n",
       "7    8      cities        city   NOUN  NNS                                            Number=Plur     9    compound  None   \n",
       "8    9  exhibition  exhibition   NOUN   NN                                            Number=Sing     4         obl  None   \n",
       "9   10          of          of    ADP   IN                                                   None    11        case  None   \n",
       "10  11        1903        1903    NUM   CD                                           NumType=Card     9        nmod  None   \n",
       "11  12           .           .  PUNCT    .                                                   None     4       punct  None   \n",
       "\n",
       "                           MISC  \n",
       "0   start_char=347|end_char=350  \n",
       "1   start_char=351|end_char=357  \n",
       "2   start_char=358|end_char=361  \n",
       "3   start_char=362|end_char=371  \n",
       "4   start_char=372|end_char=381  \n",
       "5   start_char=382|end_char=385  \n",
       "6   start_char=386|end_char=393  \n",
       "7   start_char=394|end_char=400  \n",
       "8   start_char=401|end_char=411  \n",
       "9   start_char=412|end_char=414  \n",
       "10  start_char=415|end_char=419  \n",
       "11  start_char=419|end_char=420  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FORM</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>UPOS</th>\n",
       "      <th>FEATS</th>\n",
       "      <th>HEAD</th>\n",
       "      <th>DEPREL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>{74: 90}</td>\n",
       "      <td>series</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>series</td>\n",
       "      <td>series</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>{74: 92, 'Number_sing': True}</td>\n",
       "      <td>conducted</td>\n",
       "      <td>nsubjpass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>{74: 100, 'Tense_past': True, 'VerbForm_fin': True}</td>\n",
       "      <td>conducted</td>\n",
       "      <td>auxpass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>conducted</td>\n",
       "      <td>conduct</td>\n",
       "      <td>VERB</td>\n",
       "      <td>{74: 100, 'Aspect_perf': True, 'Tense_past': True, 'VerbForm_part': True}</td>\n",
       "      <td>conducted</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>alongside</td>\n",
       "      <td>alongside</td>\n",
       "      <td>ADP</td>\n",
       "      <td>{74: 85}</td>\n",
       "      <td>conducted</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>{74: 90}</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39</td>\n",
       "      <td>Dresden</td>\n",
       "      <td>Dresden</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>{74: 96, 'NounType_prop': True, 'Number_sing': True}</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>cities</td>\n",
       "      <td>city</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>{74: 92, 'Number_plur': True}</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>{74: 92, 'Number_sing': True}</td>\n",
       "      <td>alongside</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>{74: 85}</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>68</td>\n",
       "      <td>1903</td>\n",
       "      <td>1903</td>\n",
       "      <td>NUM</td>\n",
       "      <td>{74: 93, 'NumType_card': True}</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>{74: 97, 'PunctType_peri': True}</td>\n",
       "      <td>conducted</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID        FORM       LEMMA   UPOS                                                                      FEATS        HEAD     DEPREL\n",
       "0    0         The         the    DET                                                                   {74: 90}      series        det\n",
       "1    4      series      series   NOUN                                              {74: 92, 'Number_sing': True}   conducted  nsubjpass\n",
       "2   11         was          be    AUX                        {74: 100, 'Tense_past': True, 'VerbForm_fin': True}   conducted    auxpass\n",
       "3   15   conducted     conduct   VERB  {74: 100, 'Aspect_perf': True, 'Tense_past': True, 'VerbForm_part': True}   conducted       ROOT\n",
       "4   25   alongside   alongside    ADP                                                                   {74: 85}   conducted       prep\n",
       "5   35         the         the    DET                                                                   {74: 90}  exhibition        det\n",
       "6   39     Dresden     Dresden  PROPN                       {74: 96, 'NounType_prop': True, 'Number_sing': True}  exhibition       amod\n",
       "7   47      cities        city   NOUN                                              {74: 92, 'Number_plur': True}  exhibition   compound\n",
       "8   54  exhibition  exhibition   NOUN                                              {74: 92, 'Number_sing': True}   alongside       pobj\n",
       "9   65          of          of    ADP                                                                   {74: 85}  exhibition       prep\n",
       "10  68        1903        1903    NUM                                             {74: 93, 'NumType_card': True}          of       pobj\n",
       "11  72           .           .  PUNCT                                           {74: 97, 'PunctType_peri': True}   conducted      punct"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the text \n",
    "# Pipeline 1: stanza\n",
    "nlp_stanza = stanza.Pipeline('en', use_gpu=False, verbose = False)\n",
    "doc_stanza = nlp_stanza(simmeltext)\n",
    "\n",
    "# Pipeline 2: spaCy\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\", exclude = ['textcat', 'tok2vec', 'transformer', 'attribute_ruler']) # Already done in the beginning but doing it again for ease of understanding the code\n",
    "doc_spacy = nlp_spacy(simmeltext)\n",
    "\n",
    "# Print out the results from both pipelines\n",
    "\n",
    "# Chosen sentence : \"The series was conducted alongside the Dresden cities exhibition of 1903\"\n",
    "sentence_stanza = doc_stanza.sentences[2]\n",
    "\n",
    "columns = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']\n",
    "tagged = [(word.id, word.text, word.lemma, word.upos, word.xpos, word.feats, word.head, word.deprel, word.deps, word.misc) for word in sentence_stanza.words]\n",
    "results_stanza = pd.DataFrame(tagged, columns=columns)\n",
    "\n",
    "spacy_sent_list = [sent.text for sent in doc_spacy.sents]\n",
    "sentence_spacy = spacy_sent_list[5]\n",
    "\n",
    "columns = ['ID', 'FORM', 'LEMMA', 'UPOS', 'FEATS', 'HEAD', 'DEPREL']\n",
    "tagged = [(word.idx, word.text, word.lemma_, word.pos_, nlp_spacy.vocab.morphology.tag_map[word.tag_], word.head, word.dep_) for word in nlp_spacy(sentence_spacy)]\n",
    "results_spacy = pd.DataFrame(tagged, columns=columns)\n",
    "\n",
    "\n",
    "display(results_stanza)\n",
    "display(results_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dedc801e7a0a49368cda796cf5d96567-0\" class=\"displacy\" width=\"1370\" height=\"436.2\" direction=\"ltr\" style=\"max-width: none; height: 436.2px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">series</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">conducted</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">alongside</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">Dresden</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">cities</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">exhibition</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"346.2\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">1903.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-0\" stroke-width=\"1.2px\" d=\"M70,301.2 C70,241.2 150.0,241.2 150.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,303.2 L67,296.2 73,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-1\" stroke-width=\"1.2px\" d=\"M190,301.2 C190,181.2 395.0,181.2 395.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj:pass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,303.2 L187,296.2 193,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-2\" stroke-width=\"1.2px\" d=\"M310,301.2 C310,241.2 390.0,241.2 390.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux:pass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,303.2 L307,296.2 313,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-3\" stroke-width=\"1.2px\" d=\"M550,301.2 C550,61.19999999999999 1005.0,61.19999999999999 1005.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550,303.2 L547,296.2 553,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-4\" stroke-width=\"1.2px\" d=\"M670,301.2 C670,121.19999999999999 1000.0,121.19999999999999 1000.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,303.2 L667,296.2 673,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-5\" stroke-width=\"1.2px\" d=\"M790,301.2 C790,241.2 870.0,241.2 870.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,303.2 L787,296.2 793,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-6\" stroke-width=\"1.2px\" d=\"M910,301.2 C910,241.2 990.0,241.2 990.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910,303.2 L907,296.2 913,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-7\" stroke-width=\"1.2px\" d=\"M430,301.2 C430,1.1999999999999886 1010.0,1.1999999999999886 1010.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1010.0,303.2 L1013.0,296.2 1007.0,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-8\" stroke-width=\"1.2px\" d=\"M1150,301.2 C1150,241.2 1230.0,241.2 1230.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1150,303.2 L1147,296.2 1153,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dedc801e7a0a49368cda796cf5d96567-0-9\" stroke-width=\"1.2px\" d=\"M1030,301.2 C1030,181.2 1235.0,181.2 1235.0,301.2\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dedc801e7a0a49368cda796cf5d96567-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1235.0,303.2 L1238.0,296.2 1232.0,296.2\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snlp = stanza.Pipeline(lang=\"en\", use_gpu=False, verbose = False)\n",
    "nlp = StanzaLanguage(snlp)\n",
    "doc = nlp('The series was conducted alongside the Dresden cities exhibition of 1903.')\n",
    "\n",
    "displacy.render(doc, jupyter=True, options={'distance':120, 'arrow_stroke':1.2, 'arrow_width':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHPPCgMaQTux"
   },
   "source": [
    "### Task 6.2 (0.5 points)\n",
    "\n",
    "**Look at the output from both pipelines. Are the results correct and do the pipelines have the same output? If no, provide the examples of the mistakes and differences.** \n",
    "\n",
    "Answer: I couldn't find any mistakes in the Stanza output, all the tags and dependency relations seem to be correct. Maybe only the morphological features of Dresden is a bit worse than spacy, since stanza wasn't able to state it as 'proper noun' under the 'feats' column, yet it managed to state it under 'upos' column. Not a big problem.\n",
    "\n",
    "Spacy's output was similar yet not exactly the same as stanza's. Both pipelines returned correct output lemmas and POS tags, yet spacy fared a bit worse in finding morphological tags and dependency relations compared to stanza. For example; stanza gave a bit more granularity for the words 'was' and 'conducted', while spacy only returned their verb form and tense. \n",
    "\n",
    "You can find the differences for dependency parsing results and the mistakes that spacy made in its output in my answer below.\n",
    "\n",
    "**What is the difference between a POS tag and morphological tag?**\n",
    "\n",
    "Answer: A POS tag refers to the token's role in a sentence. This role can be noun, verb, adjective etc. A morphological tag refers to the meaning of a morpheme in a word. A word can be composed of several morphemes and thus can take several morphological tags. These tags can indicate singularity/plurality(ex:shark vs sharks), tense, active/passive voice etc for the word.\n",
    "\n",
    "**What is the difference between tagging and parsing?**\n",
    "\n",
    "Answer: Tagging is for a detecting the particular meanings that individual tokens carry; while parsing is for understanding the structure of words in a sentence, ie understanding in what order they form the sentence, the syntactic dependency relations between them. If we use a simplification, we can think of parsing as understanding the flow of words in a sentence.\n",
    "\n",
    "**Analyze the dependency parsing result from both pipelines. Does the results make sense? Briefly describe the meaning behind the relations.**\n",
    "\n",
    "Answer: I have drawn the graph for the dependency relations above. \n",
    "\n",
    "I used the stanza output for this since it seemed to be more correct than the one returned by spacy. Both pipelines found the root correctly; but stanza managed to understand that the words 'alongside' and 'cities' refers to the word 'exhibition' as their head, and the head for 'Dresden' is 'cities', not 'exhibition' as spacy outputs.\n",
    "\n",
    "With regards to relations, I understood that the root of the dependency relations start from the verb 'conducted' and this particular verb links the first(the series...) and second half(...dresden cities exhibition) of the sentence together.\n",
    "\n",
    "**Is one pipeline better than the other based on the output of one sentence?**\n",
    "\n",
    "Answer: Stanza is better. It's not only more accurate with dependency parsing, but also easier to use compared to spacy, with few lines of code required to get the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs1VDYjwQgqZ"
   },
   "source": [
    "## Task 7: Statistics (1 point)\n",
    "\n",
    "In your processed output (choose output from only one of the pipelines), compute and print out (in a human readable format) the following statistics: \n",
    "* POS tag frequency for each tag (in descending order) \n",
    "\n",
    "* 50 most frequent lemmas \n",
    "* 10 least frequent lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "RsgA9FaXQgFx",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Count\n",
       "NOUN     158\n",
       "ADP      100\n",
       "DET       90\n",
       "PUNCT     63\n",
       "ADJ       51\n",
       "VERB      51\n",
       "PRON      26\n",
       "AUX       23\n",
       "PROPN     20\n",
       "CCONJ     20\n",
       "ADV       17\n",
       "PART      14\n",
       "NUM        5\n",
       "SCONJ      4\n",
       "X          2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "\n",
      "Most frequent 50 lemmas are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individual</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Simmel</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dweller</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metropolis</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>establish</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rural</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>such</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lecture</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>originally</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>century</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nature</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>;</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>each</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Count\n",
       "the              66\n",
       "of               34\n",
       ",                28\n",
       "in               19\n",
       ".                18\n",
       "to               18\n",
       "and              16\n",
       "a                13\n",
       "be               12\n",
       "he               10\n",
       "city              9\n",
       "individual        9\n",
       "which             8\n",
       "as                8\n",
       "Simmel            7\n",
       "life              7\n",
       "on                6\n",
       "psychology        6\n",
       "man               5\n",
       "with              5\n",
       "for               4\n",
       "dweller           4\n",
       "from              4\n",
       "(                 4\n",
       "all               4\n",
       "have              4\n",
       "metropolis        4\n",
       ")                 4\n",
       "that              3\n",
       "establish         3\n",
       "but               3\n",
       "rural             3\n",
       "relationship      3\n",
       "series            3\n",
       "such              3\n",
       "lecture           3\n",
       "originally        3\n",
       "'s                3\n",
       "essay             3\n",
       "work              3\n",
       "make              3\n",
       "by                3\n",
       "same              3\n",
       "one               3\n",
       "may               3\n",
       "century           3\n",
       "nature            3\n",
       ";                 3\n",
       "more              2\n",
       "each              2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "\n",
      "Least frequent 10 lemmas are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>etc.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currency</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>become</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>within</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extent</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Count\n",
       "etc.          1\n",
       "only          1\n",
       "currency      1\n",
       "art           1\n",
       "money         1\n",
       "exchange      1\n",
       "become        1\n",
       "medium        1\n",
       "within        1\n",
       "extent        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute and print out POS tag frequency \n",
    "nlp = stanza.Pipeline('en', use_gpu=False, verbose = False)\n",
    "doc = nlp(simmeltext)\n",
    "\n",
    "sentences = doc.sentences\n",
    "\n",
    "stats_pos = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.upos in stats_pos.keys():\n",
    "            stats_pos[word.upos] += 1\n",
    "        else:\n",
    "            stats_pos[word.upos] = 1\n",
    "            \n",
    "pos = {k:[v] for k,v in stats_pos.items()}\n",
    "pos_df = pd.DataFrame.from_dict(pos).T\n",
    "pos_df.rename(columns = {0:'Count'}, inplace = True)\n",
    "pos_df.sort_values(by = 'Count', ascending = False, inplace=True) \n",
    "\n",
    "display(pos_df)\n",
    "print('=========')\n",
    "print()\n",
    "# Compute and print out 50 most frequent lemmas \n",
    "stats_lemmas = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.lemma in stats_lemmas.keys():\n",
    "            stats_lemmas[word.lemma] += 1\n",
    "        else:\n",
    "            stats_lemmas[word.lemma] = 1\n",
    "\n",
    "stats_lemmas = {k:[v] for k,v in stats_lemmas.items()}\n",
    "lemmas_df = pd.DataFrame.from_dict(stats_lemmas).T\n",
    "lemmas_df.rename(columns = {0:'Count'}, inplace = True)\n",
    "lemmas_df.sort_values(by = 'Count', ascending = False, inplace=True) \n",
    "\n",
    "top_50 = lemmas_df.head(50)\n",
    "print('Most frequent 50 lemmas are:')\n",
    "display(top_50)\n",
    "print('=========')\n",
    "print()\n",
    "# Compute and print out 10 least frequent lemmas \n",
    "least_10 = lemmas_df.tail(10)\n",
    "print('Least frequent 10 lemmas are:')\n",
    "display(least_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTXDnViOQ8pp"
   },
   "source": [
    "## Bonus Task: WordCloud (1 point) \n",
    "\n",
    "Wordcloud gives us a visual representation of the most common words in the data. Visualisation is key to understanding whether we are on the right track with preprocessing, it allows us to verify if we need more preprocessing before further analysing the text data. \n",
    "\n",
    "**Your task is to create three wordclouds. One wordcloud should be created before any preprocessing is done to the text data and other two is created from the preprocessed text data. \n",
    "Do suitable preprocessing for tasks described in 4.2. This means:**\n",
    "1. preprocess data, so we could train a neural network that will capture the meaning of words. \n",
    "2. preprocess data, so we could train a system that detects named entities.\n",
    "\n",
    "\n",
    "Python has a massive number of open libraries for drawing wordclouds. You can use Andreas Mueller's [wordcloud](http://amueller.github.io/word_cloud/) library to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Q1sfiWnxZ7G9"
   },
   "outputs": [],
   "source": [
    "# create the first wordcloud from raw text data\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HSi7D4GvY1JV"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data for task 1\n",
    "...\n",
    "\n",
    "# create the second wordcloud\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bxcn5QY--AeM"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data for task 2\n",
    "...\n",
    "\n",
    "# create the third wordcloud\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mkTT_veaCVv"
   },
   "source": [
    "**What are the differences between these wordclouds (provide examples)? Can you say from the wordclouds if the preprocessing was enough for the tasks?**\n",
    "\n",
    "Answer: TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
