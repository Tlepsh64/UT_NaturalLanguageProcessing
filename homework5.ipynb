{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CanberkOzen homework5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB1YDdMM8TIe"
      },
      "source": [
        "# Homework 5. Sequence Tagging with LSTM\n",
        "\n",
        "Welcome to Homework 5! \n",
        "\n",
        "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _six_.\n",
        "\n",
        "The **grading** for each task is the following:\n",
        "- correct answer - **full points**\n",
        "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
        "- no answer or completely wrong solution - **no points**\n",
        "\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
        "\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
        "\n",
        "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
        "\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
        "\n",
        "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRSVyJhH9Xt"
      },
      "source": [
        "## Task 1. Download resourses for your language (0.5 points)\n",
        "\n",
        "In this homework, you are going to improve the pos tagger for your native language that we have built during the Lab. In particular, you are going to add a character level model to capture the inner structure of the word. This should help to better predict a correct tag. If there are no available resources for your language, you can choose any other language. \n",
        "\n",
        "__What is your native language?__\n",
        "\n",
        "<font color='red'>I kinda have two, but for the sake of simplicity let me say it is Turkish.</font>\n",
        "\n",
        "To start with, import all the packages below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yygt6gOJevIr"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIDOXHRpIWAH"
      },
      "source": [
        "Get the pretrained word vectors for your native language from [Fasttext](https://fasttext.cc/docs/en/crawl-vectors.html). For this model, you need to choose __text__ vectors (not bin!). If you don't have them available for your language, you can choose any other language that you want. Put the link instead of the `...` in the cell below.\n",
        "\n",
        "__Are there FastText vectors available for your language? If yes, provide the link to it below.__\n",
        "\n",
        "<font color='red'>https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1dBJUSQe6eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e58a06-4bcf-49d5-aadf-9f668dca2a75"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-20 20:16:25--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1261500728 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.tr.300.vec.gz’\n",
            "\n",
            "cc.tr.300.vec.gz    100%[===================>]   1.17G  21.5MB/s    in 59s     \n",
            "\n",
            "2021-04-20 20:17:25 (20.4 MB/s) - ‘cc.tr.300.vec.gz’ saved [1261500728/1261500728]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dmuzm7tD4EF"
      },
      "source": [
        "Replace `...` with the filename of your vector in the first line. Put the same name but without a `.gz` extension instead of the `...` in the last line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3TIUs0fc0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cacef5-ffc5-48e8-c07c-8bcd815103c7"
      },
      "source": [
        "!gunzip cc.tr.300.vec.gz\n",
        "!mkdir vector_cache/\n",
        "!mv cc.tr.300.vec vector_cache/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxuSASQaJNAz"
      },
      "source": [
        "Next, we need the data to train on. For this task, we are going to use [Universal Dependencies](https://universaldependencies.org/) data. It has labelled corpora for morphological tagging and syntax parsing for over than 70 languages. You need to choose your language from the official UD page, choose the treebank that you like and follow the GitHub link to it. Then, from GitHub, copy the link from the green \"Clone or download\" button and replace it in the cell below. \n",
        "\n",
        "Also, replace the name of your treebank in the `!mv` command.\n",
        "\n",
        "For example, if I choose the EDT treebank for Estonian from [here](https://universaldependencies.org/#estonian-treebanks), the GitHub link is going to be `https://github.com/UniversalDependencies/UD_Estonian-EDT.git` and the name of the treebank is `UD_Estonian-EDT`, which is the name of the repository.\n",
        "\n",
        "__Is there a UD treebank available for your language? If yes, provide the link to it below.__\n",
        "\n",
        "<font color='red'>https://github.com/UniversalDependencies/UD_Turkish-BOUN.git</font>\n",
        "\n",
        "Replace the `...` with the GitHub link to the repository that you've chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJBLZXIEgbbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c0d410-85e8-4626-db67-acec2e3865ff"
      },
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_Turkish-BOUN.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UD_Turkish-BOUN'...\n",
            "remote: Enumerating objects: 331, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 331 (delta 62), reused 62 (delta 27), pack-reused 227\u001b[K\n",
            "Receiving objects: 100% (331/331), 6.54 MiB | 23.67 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n0J8ZoFEoMb"
      },
      "source": [
        "Replace the `...` with the name of the treebank that you've chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJqCd9yQggJn"
      },
      "source": [
        "!mkdir data/\n",
        "!mv UD_Turkish-BOUN data/"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KqQMYpKzUJ"
      },
      "source": [
        "This part is moslty the same as in the [Lab 6](). \n",
        "\n",
        "**Don't forget to change the `VEC_PATH` and `DATA_PATH` variables to match your data!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb4fsehMgkBd"
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') / 'cc.tr.300.vec'\n",
        "DATA_PATH = Path('data') / 'UD_Turkish-BOUN'\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5DyDzC6ORzY"
      },
      "source": [
        "You can consult the Lab materials if you have any questions about the vocab classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGsPh253hDIs"
      },
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, idx=0, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.idx = idx\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "\n",
        "    def unmap(self, ids):\n",
        "        return [self.id2unit(idx) for idx in ids]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMvi6s3lsFA3"
      },
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9mn3bWHhXER"
      },
      "source": [
        "class WordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        if self.lower:\n",
        "            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n",
        "        else:\n",
        "            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n",
        "\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_dTkVH7GBwO"
      },
      "source": [
        "Here, we introduce a character vocab that is going to store the mappings for individual characters rather than words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgIck3Z61HY8"
      },
      "source": [
        "class CharVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        counter = Counter([c for sent in self.data for w in sent for c in w[self.idx]])\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: (counter[k], k), reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPOQxl5mPdK8"
      },
      "source": [
        "You can consult the Lab materials if you have any questions about building the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOefas-mheLI"
      },
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8i2PuW6iafP"
      },
      "source": [
        "FIELD_NUM = 10\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, word: List[str]):\n",
        "        self._id = word[0]\n",
        "        self._text = word[1]\n",
        "        self._lemma = word[2]\n",
        "        self._upos = word[3]\n",
        "        self._xpos = word[4]\n",
        "        self._feats = word[5]\n",
        "        self._head = word[6]\n",
        "        self._deprel = word[7]\n",
        "        self._deps = word[8]\n",
        "        self._misc = word[9]\n",
        "\n",
        "    @property\n",
        "    def id(self):\n",
        "        return self._id\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def lemma(self):\n",
        "        return self._lemma\n",
        "\n",
        "    @property\n",
        "    def upos(self):\n",
        "        return self._upos\n",
        "\n",
        "    @property\n",
        "    def xpos(self):\n",
        "        return self._xpos\n",
        "\n",
        "    @property\n",
        "    def feats(self):\n",
        "        return self._feats\n",
        "\n",
        "    @property\n",
        "    def head(self):\n",
        "        return self._head\n",
        "\n",
        "    @property\n",
        "    def deprel(self):\n",
        "        return self._deprel\n",
        "\n",
        "    @property\n",
        "    def deps(self):\n",
        "        return self._deps\n",
        "\n",
        "    @property\n",
        "    def misc(self):\n",
        "        return self._misc\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words: List[List[str]]):\n",
        "        self._words = [Word(w) for w in words]\n",
        "\n",
        "    @property\n",
        "    def words(self):\n",
        "        return self._words\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, file_path):\n",
        "        self._sentences = []\n",
        "        self.load_conll(open(file_path, encoding='utf-8'))\n",
        "\n",
        "\n",
        "    def load_conll(self, f, ignore_gapping=True):\n",
        "        \"\"\" Load the file or string into the CoNLL-U format data.\n",
        "        Input: file or string reader, where the data is in CoNLL-U format.\n",
        "        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n",
        "        all fields of a token.\n",
        "\n",
        "        Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/utils/conll.py\n",
        "        Stanza is released under the Apache License, Version 2.0.\n",
        "        \"\"\"\n",
        "        # f is open() or io.StringIO()\n",
        "        doc, sent = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if len(line) == 0:\n",
        "                if len(sent) > 0:\n",
        "                    doc.append(Sentence(sent))\n",
        "                    sent = []\n",
        "            else:\n",
        "                if line.startswith('#'): # skip comment line\n",
        "                    continue\n",
        "                array = line.split('\\t')\n",
        "                if ignore_gapping and '.' in array[0]:\n",
        "                    continue\n",
        "                assert len(array) == FIELD_NUM, \\\n",
        "                        f\"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.\"\n",
        "                sent += [array]\n",
        "        if len(sent) > 0:\n",
        "            doc.append(Sentence(sent))\n",
        "        self._sentences = doc\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def sentences(self):\n",
        "        return self._sentences\n",
        "\n",
        "\n",
        "    def get(self, fields, as_sentences=False):\n",
        "        \"\"\"Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/models/common/doc.py\n",
        "        Stanza is released under the Apache License, Version 2.0.\n",
        "        \"\"\"\n",
        "        assert isinstance(fields, list), \"Must provide field names as a list.\"\n",
        "        assert len(fields) >= 1, \"Must have at least one field.\"\n",
        "\n",
        "        results = []\n",
        "        for sentence in self.sentences:\n",
        "            cursent = []\n",
        "            units = sentence.words\n",
        "            for unit in units:\n",
        "                if len(fields) == 1:\n",
        "                    cursent += [getattr(unit, fields[0])]\n",
        "                else:\n",
        "                    cursent += [[getattr(unit, field) for field in fields]]\n",
        "\n",
        "            # decide whether append the results as a sentence or a whole list\n",
        "            if as_sentences:\n",
        "                results.append(cursent)\n",
        "            else:\n",
        "                results += cursent\n",
        "        return results"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azsMK8kTSZfO"
      },
      "source": [
        "For the dataset, we are going to add the new `CharVocab` and preprocess each word character by character with it. For example, if you have a character vocabulary like this:\n",
        "\n",
        "`{'a': 0, 'b': 1, ..., 'y': 24, 'z': 25, 'A': 26, 'B': 27, ..., 'Y': 50, 'Z': 51}`\n",
        "\n",
        "Then a sentence `['I', 'like', 'cats']` is going to be transformed into `[[35], [11, 8, 10, 4], [2, 0, 19, 18]]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7UVFATAhsEV"
      },
      "source": [
        "class CONLLUDataset(Dataset):\n",
        "    def __init__(self, doc: Document, pretrain: Pretrain, \n",
        "                 vocab: Dict[str, BaseVocab] = None, test: bool = False):\n",
        "        self.pretrain_vocab = pretrain.vocab\n",
        "        self.test = test\n",
        "        data = self.load_doc(doc)\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab = self.init_vocab(data)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.data = self.preprocess(data, self.vocab, self.pretrain_vocab)\n",
        "\n",
        "    def init_vocab(self, data: List) -> Dict:\n",
        "        wordvocab = WordVocab(data, idx=0)\n",
        "        charvocab = CharVocab(data, idx=0)\n",
        "        uposvocab = WordVocab(data, idx=1)\n",
        "        vocab = {\n",
        "            'word': wordvocab,\n",
        "            'char': charvocab,\n",
        "            'upos': uposvocab}\n",
        "        return vocab\n",
        "\n",
        "    def preprocess(self, data: List, vocab: Dict[str, BaseVocab], \n",
        "                   pretrain_vocab: PretrainedWordVocab) -> List[List[int]]:\n",
        "        processed = []\n",
        "        for sent in data:\n",
        "            processed_sent = [vocab['word'].map([w[0] for w in sent])]\n",
        "            processed_sent += [[vocab['char'].map([char for char in w[0]]) for w in sent]]\n",
        "            processed_sent += [vocab['upos'].map([w[1] for w in sent])]\n",
        "            processed_sent += [pretrain_vocab.map([w[0].lower() for w in sent])]\n",
        "            processed.append(processed_sent)\n",
        "        return processed\n",
        "        \n",
        "    def load_doc(self, doc: Document) -> List:\n",
        "        data = doc.get(['text', 'upos'], as_sentences=True)\n",
        "        return data\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9L6KBcOkVu1"
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLhAK0nzIOhI"
      },
      "source": [
        "Put the correct path to your train file here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QraxfFjKs-jZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288be7b7-5393-42a9-c334-f83ed0f47a47"
      },
      "source": [
        "train_doc = Document(DATA_PATH / 'tr_boun-ud-train.conllu')\n",
        "train_dataset = CONLLUDataset(train_doc, pretrain)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading pretrained vectors from vector_cache/cc.tr.300.vec...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4hOIwqtIUQq"
      },
      "source": [
        "Put the correct path to your dev file here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdk0QJJheLpB"
      },
      "source": [
        "vocab = train_dataset.vocab\n",
        "dev_doc = Document(DATA_PATH / 'tr_boun-ud-dev.conllu')\n",
        "dev_dataset = CONLLUDataset(dev_doc, pretrain, vocab=vocab, test=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPFocs3RIYTA"
      },
      "source": [
        "You can look inside the first sentence to see how the preprocessed data looks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBZSDFv6t_-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aff4650-008b-4fab-8ea0-5a6dcf877fcd"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4929, 9269, 292, 1305, 2],\n",
              " [[47, 66, 69, 76],\n",
              "  [14, 8, 7, 8, 5, 10, 2, 14, 8, 21],\n",
              "  [14, 8, 7, 8, 5, 10, 2],\n",
              "  [14, 8, 21],\n",
              "  [20]],\n",
              " [12, 14, 2, 13, 4],\n",
              " [9739, 1, 102, 15712, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1_mmSj9Ipzv"
      },
      "source": [
        "We are going to pad the characters and save the original lengths for each word in a sentence to reconstruct the correct order later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbhf2uYHvcJy"
      },
      "source": [
        "def pad_collate(batch):\n",
        "    (sents, chars, upos, pretrained) = zip(*batch)\n",
        "\n",
        "    sent_lens = [len(s) for s in sents]\n",
        "    word_lens = [len(c) for w in chars for c in w]\n",
        "\n",
        "    sents = [torch.LongTensor(w).to(device) for w in sents]\n",
        "    chars = [torch.LongTensor(c).to(device) for w in chars for c in w]\n",
        "    upos = [torch.LongTensor(u).to(device) for u in upos]\n",
        "    pretrained = [torch.LongTensor(p).to(device) for p in pretrained]\n",
        "\n",
        "    sent_pad = pad_sequence(sents, batch_first=True, padding_value=PAD_ID)\n",
        "    chars_pad = pad_sequence(chars, batch_first=True, padding_value=PAD_ID)\n",
        "    upos_pad = pad_sequence(upos, batch_first=True, padding_value=PAD_ID)\n",
        "    pretrained_pad = pad_sequence(pretrained, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    sent_pad = sent_pad.to(device)\n",
        "    chars_pad = chars_pad.to(device)\n",
        "    upos_pad = upos_pad.to(device)\n",
        "    pretrained_pad = pretrained_pad.to(device)\n",
        "\n",
        "    return sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMNbcGrQ7dGd"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li5RRsb6IrJO"
      },
      "source": [
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLNjeeQUa0U3"
      },
      "source": [
        "## Task 2. Build a character-level LSTM model (3 points)\n",
        "\n",
        "You already know that we can have a vector representation of a word that is learned from its context. We can use these vectors to capture sematical relations between the words. \n",
        "\n",
        "In addition to that we can learn an inner representation of a word, or its character-level embedding. This can help to capture morphological information about a word.\n",
        "\n",
        "We can do it by building another LSTM model and taking its last hidden state which is going to be the character-level representation of a word. The input to the model is going to be a stream for characters for each word in a batch.\n",
        "\n",
        "### Task 2.1. Define an Embedding layer (0.5 points)\n",
        "\n",
        "Create an [`nn.Embedding`](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) layer. The input size, or the number of embeddings, should be the size of our vocabulary (`CONLLUDataset` class shows how to access them). The output size, or the size of each vector, is `char_emb_dim`. Padding index is `0`. \n",
        "\n",
        "### Task 2.2. Define an LSTM layer (0.5 points)\n",
        "\n",
        "Create an [`nn.LSTM`](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM) layer. The input size should be the output size of the embedding layer. The hidden size is `char_hidden_dim`. The number of layers is `char_num_layers`. Set the `batch_first` parameter to `True`. Make the droupout to be `0` if number of layers is `1` or `droupout` otherwise.\n",
        "\n",
        "### Task 2.3. Embed the input (0.5 points)\n",
        "\n",
        "Now, you will implement a forward pass.\n",
        "\n",
        "Run the character input (`chars_pad`) through the embedding layer. Apply the dropout to the embeddings and save the result into the `char_emb` variable.\n",
        "\n",
        "### Task 2.4. Apply the LSTM layer (0.5 points)\n",
        "\n",
        "Put the embeddings into the LSTM layer. You are going to save only the last hidden state that is going to be the representation of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSY9a1DGHU8v"
      },
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, BaseVocab], char_emb_dim: int,\n",
        "                 char_hidden_dim: int, char_num_layers: int, dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        ### Task 2.1 starts here ###\n",
        "        self.char_emb = nn.Embedding(len(vocab['char']), char_emb_dim, padding_idx = 0)\n",
        "        ### Task 2.1 ends here ###\n",
        "\n",
        "        ### Task 2.2 starts here ###\n",
        "        if char_num_layers == 1:\n",
        "          dropout = 0\n",
        "\n",
        "        self.char_lstm = nn.LSTM(char_emb_dim, char_hidden_dim, char_num_layers, batch_first = True, dropout = dropout)\n",
        "        self.char_lstm_h_init = nn.Parameter(torch.zeros(char_num_layers, 1, char_hidden_dim))\n",
        "        self.char_lstm_c_init = nn.Parameter(torch.zeros(char_num_layers, 1, char_hidden_dim))\n",
        "        ### Task 2.2 ends here ###\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, chars_pad, sent_lens, word_lens):\n",
        "        ### Task 2.3 starts here ###\n",
        "        char_emb = self.dropout(self.char_emb(chars_pad))\n",
        "        ### Task 2.3 ends here ###\n",
        "\n",
        "        batch_size = char_emb.size(0)\n",
        "        char_emb = pack_padded_sequence(char_emb, word_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        ### Task 2.4 starts here ###\n",
        "        _, (h, _) = self.char_lstm(\n",
        "            char_emb,\n",
        "            (self.char_lstm_h_init.expand(char_num_layers, batch_size, char_hidden_dim).contiguous(),\n",
        "             self.char_lstm_c_init.expand(char_num_layers, batch_size, char_hidden_dim).contiguous())\n",
        "        )\n",
        "        ### Task 2.4 ends here ###\n",
        "\n",
        "        # Remove an empty dimension\n",
        "        result = h.squeeze(0)\n",
        "        # Chunk the output back into words\n",
        "        result = pack_sequence(result.split(sent_lens), enforce_sorted=False)\n",
        "\n",
        "        return result"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXaj5bXEQxAw"
      },
      "source": [
        "### Task 2.5. Incorporate the CharLSTM into the Tagger model (0.5 points)\n",
        "\n",
        "Now your new model should have the following architecture:\n",
        "\n",
        "![img](https://github.com/501Good/tartu-nlp-2020/blob/master/homeworks/hw4/img1.png?raw=1)\n",
        "\n",
        "Initialize the CharLSTM model that you've just created. Pass `vocab`, `char_emb_dim`, `char_hidden_dim`, `char_num_layers`, `dropout` to the class constructor. Save it to `self.char_model`.\n",
        "\n",
        "Create another [`nn.Linear`](https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear) layer to transform the character representations. It should work similar to the linear layer for the transformation of the pretrained vectors. The input size should be `char_hidden_dim` and the output size is `transformed_dim`. Also, set the `bias` parameter to `False`.\n",
        "\n",
        "Add the `transformed_dim` to the `input_size`.\n",
        "\n",
        "### Task 2.6. Get the character embeddings in the forward pass (0.5 points)\n",
        "\n",
        "Perform the forward pass on your CharLSTM and save the results into the `char_emb` variable. Pass all the appropriate parameters to the CharLSTM.\n",
        "\n",
        "Apply the dropout to the `char_emb`. After that transform it with the `self.trans_char` linear layer. \n",
        "\n",
        "**NB!** Since the output of the CharLSTM is a `PackedSequence` object, you need to apply the dropout to `char_emb.data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N-xVFW28FD3"
      },
      "source": [
        "class Tagger(nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, BaseVocab], word_emb_dim: int,\n",
        "                 char_emb_dim: int, transformed_dim: int, emb_matrix: np.ndarray,\n",
        "                 hidden_dim: int, char_hidden_dim: int,\n",
        "                 upos_clf_hidden_dim: int, num_layers: int, char_num_layers: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        input_size = 0\n",
        "\n",
        "        self.word_emb = nn.Embedding(len(vocab['word']), word_emb_dim, padding_idx=0)\n",
        "        input_size += word_emb_dim\n",
        "\n",
        "        ### Task 2.5 starts here ###\n",
        "        self.char_model = CharLSTM(vocab, char_emb_dim, char_hidden_dim, char_num_layers, dropout)\n",
        "        self.trans_char = nn.Linear(char_hidden_dim, transformed_dim, bias=False)\n",
        "        input_size += transformed_dim\n",
        "        ### Task 2.5 ends here ###\n",
        "\n",
        "        self.pretrained_emb = nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True)\n",
        "        self.trans_pretrained = nn.Linear(emb_matrix.shape[1], transformed_dim, bias=False)\n",
        "        input_size += transformed_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.lstm_h_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "        self.lstm_c_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "\n",
        "        self.upos_hid = nn.Linear(2* hidden_dim, upos_clf_hidden_dim)\n",
        "        self.upos_clf = nn.Linear(upos_clf_hidden_dim, len(vocab['upos']))\n",
        "\n",
        "        self.crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens):\n",
        "        inputs = []\n",
        "\n",
        "        word_emb = self.word_emb(sent_pad)\n",
        "        inputs += [word_emb]\n",
        "\n",
        "        pretrained_emb = self.pretrained_emb(pretrained_pad)\n",
        "        pretrained_emb = self.trans_pretrained(pretrained_emb)\n",
        "        inputs += [pretrained_emb]\n",
        "\n",
        "        ### Task 2.6 starts here ###\n",
        "        char_emb = self.char_model.forward(chars_pad, sent_lens, word_lens)\n",
        "        char_emb_trans = self.trans_char(self.drop(char_emb.data))\n",
        "        ### Task 2.6 ends here ###\n",
        "        # Creating a PackedSequence from the embeddings to restore the original order\n",
        "        char_emb = PackedSequence(char_emb_trans, char_emb.batch_sizes, \n",
        "                                  char_emb.sorted_indices, char_emb.unsorted_indices)\n",
        "        char_emb = pad_packed_sequence(char_emb, batch_first=True)[0]\n",
        "        inputs += [char_emb]\n",
        "\n",
        "        lstm_inputs = torch.cat([x for x in inputs], 2)\n",
        "        lstm_inputs = self.drop(lstm_inputs)\n",
        "        lstm_inputs = pack_padded_sequence(lstm_inputs, sent_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        lstm_outputs, _ = self.lstm(\n",
        "            lstm_inputs, \n",
        "            (self.lstm_h_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous(), \n",
        "             self.lstm_c_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous())\n",
        "        )\n",
        "        lstm_outputs = lstm_outputs.data\n",
        "\n",
        "        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))\n",
        "        upos_pred = self.upos_clf(self.drop(upos_hid))\n",
        "\n",
        "        pred = PackedSequence(upos_pred, lstm_inputs.batch_sizes,\n",
        "                              lstm_inputs.sorted_indices, lstm_inputs.unsorted_indices)\n",
        "        pred = pad_packed_sequence(pred, batch_first=True)[0]\n",
        "        pred = pred.max(2)[1]\n",
        "        upos = pack_padded_sequence(upos_pad, sent_lens, batch_first=True, enforce_sorted=False).data\n",
        "        loss = self.crit(upos_pred, upos)\n",
        "\n",
        "        return loss, pred"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iZlzjyF4no"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                 emb_matrix, hidden_dim, char_hidden_dim, upos_clf_hidden_dim, \n",
        "                 num_layers, char_num_layers, dropout):\n",
        "        self.vocab = vocab\n",
        "        self.model = Tagger(vocab, word_emb_dim, char_emb_dim, transformed_dim, \n",
        "                            emb_matrix, hidden_dim, char_hidden_dim, \n",
        "                            upos_clf_hidden_dim, num_layers, char_num_layers, \n",
        "                            dropout)\n",
        "        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters)\n",
        " \n",
        "    def update(self, batch, eval=False):\n",
        "        sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens = batch\n",
        "\n",
        "        if eval:\n",
        "            self.model.eval()\n",
        "        else:\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        loss, _ = self.model(sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens)\n",
        "        loss_val = loss.data.item()\n",
        "        if eval:\n",
        "            return loss_val\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "\n",
        "    def predict(self, batch):\n",
        "        sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens = batch\n",
        "\n",
        "        self.model.eval()\n",
        "        batch_size = sent_pad.size(0)\n",
        "        _, pred = self.model(sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens)\n",
        "        # Transform the indices to the pos tags\n",
        "        pred = [self.vocab['upos'].unmap(sent) for sent in pred.tolist()]\n",
        "        # Trim the predictions to their original lengths\n",
        "        pred_tokens = [[pred[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "\n",
        "        gold_upos = [vocab['upos'].unmap(upos) for upos in [upos for upos in upos_pad]]\n",
        "        gold_tokens = [[gold_upos[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "\n",
        "        return pred_tokens, gold_tokens"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAu5ZF94JMvN"
      },
      "source": [
        "word_emb_dim = 75\n",
        "char_emb_dim = 100\n",
        "transformed_dim = 125\n",
        "emb_matrix = pretrain.emb\n",
        "hidden_dim = 200\n",
        "char_hidden_dim = 400\n",
        "upos_clf_hidden_dim = 400\n",
        "num_layers = 2\n",
        "char_num_layers = 1\n",
        "dropout = 0.5"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKLCdBaJG-t"
      },
      "source": [
        "trainer = Trainer(vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                  emb_matrix, hidden_dim, char_hidden_dim, upos_clf_hidden_dim,\n",
        "                  num_layers, char_num_layers, dropout)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjJm834cJ-kZ"
      },
      "source": [
        "global_step = 0\n",
        "max_steps = 50000\n",
        "dev_score_history = []\n",
        "format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch)'\n",
        "last_best_step = 0\n",
        "\n",
        "log_step = 20\n",
        "eval_interval = 100"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT2AjVnth0_L"
      },
      "source": [
        "## Task 3. Train your model (0.5 points)\n",
        "\n",
        "Run the cell below to start training the model. After each 100 steps, it is going to print out the average training loss and dev score, which is a simple accuracy in our case. You should see the training loss decreasing and the dev score increasing.\n",
        "\n",
        "Train the model until you don't see the increase in the dev score anymore. Report the score that you've got.\n",
        "\n",
        "__My maximum dev_score is:__\n",
        "\n",
        "<font color='red'>0.924593</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5M8lS5Kibm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20e41ecb-2829-4543-bdea-441b423be1b4"
      },
      "source": [
        "train_loss = 0\n",
        "while True:\n",
        "    do_break = False\n",
        "    for batch in train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), global_step,\\\n",
        "                    max_steps, loss, duration))\n",
        "            \n",
        "        if global_step % eval_interval == 0:\n",
        "            print(\"Evaluating on dev set...\")\n",
        "            dev_preds = []\n",
        "            dev_words = []\n",
        "            dev_correct = 0\n",
        "            dev_total = 0\n",
        "            for batch in dev_loader:\n",
        "                batch_size = batch[0].size(0)\n",
        "                preds, gold = trainer.predict(batch)\n",
        "                dev_correct += sum([1 for sent in zip(preds, gold) for pair in zip(*sent) if pair[0] == pair[1]])\n",
        "                dev_total += sum([len(sent) for sent in gold])\n",
        "                dev_preds += preds\n",
        "                # Keep the original sentence\n",
        "                pred_sents = [[batch[0][i][j] for j in range(batch[4][i])] for i in range(batch_size)]\n",
        "                dev_words += [vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]]\n",
        "            \n",
        "            dev_score = dev_correct / dev_total\n",
        "            train_loss = train_loss / eval_interval\n",
        "            print(\"step {}: train_loss = {:.6f}, dev_score = {:.6f}\".format(global_step, train_loss, dev_score))\n",
        "            # Shows one prediction for a sanity check\n",
        "            print(f\"Preds: {list(zip(dev_preds[0], dev_words[0]))}\")\n",
        "            train_loss = 0\n",
        "\n",
        "        if global_step >= max_steps:\n",
        "            do_break = True\n",
        "            break\n",
        "\n",
        "        if do_break:\n",
        "            break"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-20 20:22:33: step 20/50000, loss = 1.920438 (0.039 sec/batch)\n",
            "2021-04-20 20:22:34: step 40/50000, loss = 1.512083 (0.033 sec/batch)\n",
            "2021-04-20 20:22:36: step 60/50000, loss = 1.319070 (0.026 sec/batch)\n",
            "2021-04-20 20:22:37: step 80/50000, loss = 1.254878 (0.032 sec/batch)\n",
            "2021-04-20 20:22:38: step 100/50000, loss = 1.258861 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 100: train_loss = 1.605573, dev_score = 0.636767\n",
            "Preds: [('NOUN', 'yılını'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:22:40: step 120/50000, loss = 1.141255 (0.030 sec/batch)\n",
            "2021-04-20 20:22:41: step 140/50000, loss = 0.999111 (0.033 sec/batch)\n",
            "2021-04-20 20:22:43: step 160/50000, loss = 0.939796 (0.028 sec/batch)\n",
            "2021-04-20 20:22:44: step 180/50000, loss = 0.795503 (0.025 sec/batch)\n",
            "2021-04-20 20:22:45: step 200/50000, loss = 0.807780 (0.026 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 200: train_loss = 0.944472, dev_score = 0.799984\n",
            "Preds: [('NOUN', 'Doğrusu'), ('NOUN', 'Barış'), ('PUNCT', ','), ('NOUN', 'yeşile'), ('VERB', 'olan'), ('NOUN', '<UNK>'), ('ADP', 'ancak'), ('PRON', 'burada'), ('PRON', 'biraz'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:22:47: step 220/50000, loss = 0.733816 (0.030 sec/batch)\n",
            "2021-04-20 20:22:48: step 240/50000, loss = 0.674380 (0.037 sec/batch)\n",
            "2021-04-20 20:22:50: step 260/50000, loss = 0.582556 (0.027 sec/batch)\n",
            "2021-04-20 20:22:51: step 280/50000, loss = 0.601341 (0.046 sec/batch)\n",
            "2021-04-20 20:22:52: step 300/50000, loss = 0.625550 (0.035 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 300: train_loss = 0.648208, dev_score = 0.851815\n",
            "Preds: [('ADJ', 'Güzel'), ('DET', 'bir'), ('NOUN', 'mayıs'), ('NOUN', 'günü'), ('PROPN', 'Atatürk'), ('PUNCT', ','), ('PROPN', '<UNK>'), ('CCONJ', 'da'), ('VERB', 'alarak'), ('NOUN', '<UNK>'), ('ADJ', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('VERB', 'gitti'), ('PUNCT', '.')]\n",
            "2021-04-20 20:22:54: step 320/50000, loss = 0.569515 (0.031 sec/batch)\n",
            "2021-04-20 20:22:55: step 340/50000, loss = 0.535504 (0.032 sec/batch)\n",
            "2021-04-20 20:22:57: step 360/50000, loss = 0.525842 (0.034 sec/batch)\n",
            "2021-04-20 20:22:58: step 380/50000, loss = 0.527805 (0.046 sec/batch)\n",
            "2021-04-20 20:22:59: step 400/50000, loss = 0.467493 (0.034 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 400: train_loss = 0.521493, dev_score = 0.876129\n",
            "Preds: [('PRON', 'Hepsi'), ('ADJ', 'güzel'), ('PUNCT', ','), ('ADJ', 'sevimli'), ('PUNCT', ',')]\n",
            "2021-04-20 20:23:01: step 420/50000, loss = 0.500005 (0.031 sec/batch)\n",
            "2021-04-20 20:23:02: step 440/50000, loss = 0.464056 (0.030 sec/batch)\n",
            "2021-04-20 20:23:04: step 460/50000, loss = 0.505124 (0.033 sec/batch)\n",
            "2021-04-20 20:23:05: step 480/50000, loss = 0.488641 (0.034 sec/batch)\n",
            "2021-04-20 20:23:06: step 500/50000, loss = 0.401996 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 500: train_loss = 0.465245, dev_score = 0.884344\n",
            "Preds: [('PRON', 'Biz'), ('ADV', 'hiç'), ('PUNCT', ','), ('ADJ', 'kadınlardan'), ('VERB', '<UNK>'), ('NOUN', 'ev'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:23:08: step 520/50000, loss = 0.438500 (0.033 sec/batch)\n",
            "2021-04-20 20:23:10: step 540/50000, loss = 0.475869 (0.035 sec/batch)\n",
            "2021-04-20 20:23:11: step 560/50000, loss = 0.425166 (0.036 sec/batch)\n",
            "2021-04-20 20:23:12: step 580/50000, loss = 0.420174 (0.033 sec/batch)\n",
            "2021-04-20 20:23:13: step 600/50000, loss = 0.346657 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 600: train_loss = 0.424894, dev_score = 0.895351\n",
            "Preds: [('NOUN', '<UNK>'), ('DET', 'bu'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', ','), ('ADV', 'niye'), ('DET', 'bu'), ('NOUN', '<UNK>'), ('NOUN', 'sayfasını'), ('VERB', '<UNK>'), ('PRON', 'ben'), ('PUNCT', '\"'), ('ADP', 'diye'), ('ADP', 'diye'), ('NOUN', 'test'), ('VERB', '<UNK>'), ('PUNCT', '...')]\n",
            "2021-04-20 20:23:15: step 620/50000, loss = 0.406371 (0.042 sec/batch)\n",
            "2021-04-20 20:23:17: step 640/50000, loss = 0.397873 (0.027 sec/batch)\n",
            "2021-04-20 20:23:18: step 660/50000, loss = 0.416418 (0.031 sec/batch)\n",
            "2021-04-20 20:23:19: step 680/50000, loss = 0.391825 (0.026 sec/batch)\n",
            "2021-04-20 20:23:20: step 700/50000, loss = 0.418579 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 700: train_loss = 0.396470, dev_score = 0.898636\n",
            "Preds: [('PRON', 'Bunun'), ('NOUN', 'üzerine'), ('NOUN', '<UNK>'), ('PUNCT', ','), ('PROPN', '<UNK>'), ('ADJ', 'üstüne'), ('VERB', '<UNK>'), ('CCONJ', 've'), ('VERB', '<UNK>'), ('NOUN', 'araya'), ('VERB', 'girdiği'), ('NOUN', 'andan'), ('ADP', 'itibaren'), ('ADJ', 'canlı'), ('NOUN', 'yayın'), ('VERB', 'kesildi'), ('PUNCT', '.')]\n",
            "2021-04-20 20:23:23: step 720/50000, loss = 0.340016 (0.036 sec/batch)\n",
            "2021-04-20 20:23:24: step 740/50000, loss = 0.356594 (0.033 sec/batch)\n",
            "2021-04-20 20:23:25: step 760/50000, loss = 0.475431 (0.031 sec/batch)\n",
            "2021-04-20 20:23:26: step 780/50000, loss = 0.428862 (0.032 sec/batch)\n",
            "2021-04-20 20:23:27: step 800/50000, loss = 0.329907 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 800: train_loss = 0.371402, dev_score = 0.902497\n",
            "Preds: [('PRON', 'Benim'), ('ADV', 'çok'), ('ADJ', 'yoğun'), ('DET', 'bir'), ('ADJ', 'reel'), ('NOUN', 'hayatım'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('ADJ', 'var'), ('PUNCT', '.')]\n",
            "2021-04-20 20:23:30: step 820/50000, loss = 0.384087 (0.033 sec/batch)\n",
            "2021-04-20 20:23:31: step 840/50000, loss = 0.352246 (0.034 sec/batch)\n",
            "2021-04-20 20:23:32: step 860/50000, loss = 0.351434 (0.028 sec/batch)\n",
            "2021-04-20 20:23:33: step 880/50000, loss = 0.302559 (0.030 sec/batch)\n",
            "2021-04-20 20:23:34: step 900/50000, loss = 0.341975 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 900: train_loss = 0.353843, dev_score = 0.905701\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', 'suyunun'), ('NOUN', 'hasta'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('NOUN', 'dikkat'), ('VERB', 'edilmesi'), ('PUNCT', ','), ('CCONJ', 'hatta'), ('ADJ', 'sağlam'), ('NOUN', '<UNK>'), ('ADV', 'bile'), ('NOUN', 'havuza'), ('VERB', 'girmeden'), ('ADP', 'önce'), ('NOUN', 'havuzun'), ('ADJ', 'dışında'), ('ADJ', 'ayrı'), ('NOUN', 'yerde'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', 'su'), ('CCONJ', 'ile'), ('ADV', 'iyice'), ('VERB', '<UNK>'), ('VERB', '<UNK>'), ('ADP', 'sonra'), ('NOUN', 'havuza'), ('VERB', 'girmesi'), ('NOUN', 'şart'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:23:37: step 920/50000, loss = 0.350227 (0.028 sec/batch)\n",
            "2021-04-20 20:23:38: step 940/50000, loss = 0.361542 (0.030 sec/batch)\n",
            "2021-04-20 20:23:39: step 960/50000, loss = 0.386965 (0.031 sec/batch)\n",
            "2021-04-20 20:23:40: step 980/50000, loss = 0.310390 (0.038 sec/batch)\n",
            "2021-04-20 20:23:41: step 1000/50000, loss = 0.340447 (0.034 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1000: train_loss = 0.333512, dev_score = 0.907344\n",
            "Preds: [('ADJ', 'Arkada'), ('VERB', 'kalanlar'), ('PUNCT', '.')]\n",
            "2021-04-20 20:23:44: step 1020/50000, loss = 0.300112 (0.027 sec/batch)\n",
            "2021-04-20 20:23:45: step 1040/50000, loss = 0.261083 (0.031 sec/batch)\n",
            "2021-04-20 20:23:46: step 1060/50000, loss = 0.334312 (0.032 sec/batch)\n",
            "2021-04-20 20:23:47: step 1080/50000, loss = 0.278102 (0.032 sec/batch)\n",
            "2021-04-20 20:23:48: step 1100/50000, loss = 0.305261 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1100: train_loss = 0.326448, dev_score = 0.906194\n",
            "Preds: [('NOUN', '\"Nerede'), ('VERB', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '?'), ('PUNCT', '\"'), ('VERB', 'dedi'), ('PUNCT', ','), ('PUNCT', '\"'), ('PRON', 'Sen'), ('ADV', 'kolay'), ('ADV', 'kolay'), ('ADV', 'geç'), ('VERB', '<UNK>'), ('PUNCT', '.\"')]\n",
            "2021-04-20 20:23:51: step 1120/50000, loss = 0.329055 (0.032 sec/batch)\n",
            "2021-04-20 20:23:52: step 1140/50000, loss = 0.327605 (0.032 sec/batch)\n",
            "2021-04-20 20:23:53: step 1160/50000, loss = 0.271561 (0.030 sec/batch)\n",
            "2021-04-20 20:23:54: step 1180/50000, loss = 0.318616 (0.031 sec/batch)\n",
            "2021-04-20 20:23:56: step 1200/50000, loss = 0.335947 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1200: train_loss = 0.310329, dev_score = 0.913093\n",
            "Preds: [('ADV', 'TAM'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>')]\n",
            "2021-04-20 20:23:58: step 1220/50000, loss = 0.245567 (0.033 sec/batch)\n",
            "2021-04-20 20:23:59: step 1240/50000, loss = 0.320049 (0.030 sec/batch)\n",
            "2021-04-20 20:24:00: step 1260/50000, loss = 0.279528 (0.035 sec/batch)\n",
            "2021-04-20 20:24:02: step 1280/50000, loss = 0.205630 (0.032 sec/batch)\n",
            "2021-04-20 20:24:03: step 1300/50000, loss = 0.322771 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1300: train_loss = 0.293612, dev_score = 0.913011\n",
            "Preds: [('PROPN', '<UNK>'), ('PUNCT', ','), ('ADJ', 'zavallı'), ('DET', 'bir'), ('NOUN', '<UNK>'), ('PUNCT', ';'), ('NOUN', '<UNK>'), ('DET', 'bir'), ('NOUN', '<UNK>'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('NOUN', 'adına'), ('PUNCT', ','), ('NOUN', '<UNK>'), ('NUM', '<UNK>'), ('NOUN', 'yıl'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('DET', 'bu'), ('PUNCT', '\"'), ('ADJ', 'yanlış'), ('NOUN', 'uygarlık'), ('PUNCT', '\"'), ('NOUN', '<UNK>'), ('CCONJ', 've'), ('NOUN', 'ısrarla'), ('VERB', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:24:05: step 1320/50000, loss = 0.332314 (0.030 sec/batch)\n",
            "2021-04-20 20:24:06: step 1340/50000, loss = 0.247386 (0.040 sec/batch)\n",
            "2021-04-20 20:24:08: step 1360/50000, loss = 0.328026 (0.045 sec/batch)\n",
            "2021-04-20 20:24:09: step 1380/50000, loss = 0.288568 (0.031 sec/batch)\n",
            "2021-04-20 20:24:10: step 1400/50000, loss = 0.295891 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1400: train_loss = 0.291206, dev_score = 0.913751\n",
            "Preds: [('PROPN', \"İngiltere'de\"), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('PUNCT', ','), ('NOUN', 'itfaiye'), ('NOUN', '<UNK>'), ('NOUN', 'ilgiyi'), ('VERB', 'artırmak'), ('CCONJ', 've'), ('ADV', 'daha'), ('ADV', 'fazla'), ('ADJ', 'etnik'), ('NOUN', 'gruptan'), ('NOUN', 'insanın'), ('VERB', '<UNK>'), ('NOUN', 'olanak'), ('VERB', 'vermek'), ('NOUN', 'amacıyla'), ('PROPN', 'Müslüman'), ('ADJ', 'kadınlar'), ('ADP', 'için'), ('ADJ', 'başörtülü'), ('VERB', 'üniforma'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:24:12: step 1420/50000, loss = 0.259373 (0.032 sec/batch)\n",
            "2021-04-20 20:24:13: step 1440/50000, loss = 0.285415 (0.031 sec/batch)\n",
            "2021-04-20 20:24:15: step 1460/50000, loss = 0.281045 (0.028 sec/batch)\n",
            "2021-04-20 20:24:16: step 1480/50000, loss = 0.260844 (0.033 sec/batch)\n",
            "2021-04-20 20:24:17: step 1500/50000, loss = 0.268907 (0.042 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1500: train_loss = 0.278125, dev_score = 0.914818\n",
            "Preds: [('NOUN', 'Bizimkiler'), ('PROPN', 'Nasrettin'), ('PROPN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', ':')]\n",
            "2021-04-20 20:24:19: step 1520/50000, loss = 0.293342 (0.030 sec/batch)\n",
            "2021-04-20 20:24:21: step 1540/50000, loss = 0.266938 (0.032 sec/batch)\n",
            "2021-04-20 20:24:22: step 1560/50000, loss = 0.272409 (0.028 sec/batch)\n",
            "2021-04-20 20:24:23: step 1580/50000, loss = 0.245340 (0.036 sec/batch)\n",
            "2021-04-20 20:24:24: step 1600/50000, loss = 0.234807 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1600: train_loss = 0.269279, dev_score = 0.913997\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', 'sisler'), ('VERB', 'indi'), ('PUNCT', '...')]\n",
            "2021-04-20 20:24:26: step 1620/50000, loss = 0.238582 (0.026 sec/batch)\n",
            "2021-04-20 20:24:28: step 1640/50000, loss = 0.259731 (0.031 sec/batch)\n",
            "2021-04-20 20:24:29: step 1660/50000, loss = 0.230116 (0.034 sec/batch)\n",
            "2021-04-20 20:24:30: step 1680/50000, loss = 0.208498 (0.034 sec/batch)\n",
            "2021-04-20 20:24:31: step 1700/50000, loss = 0.306334 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1700: train_loss = 0.259269, dev_score = 0.916297\n",
            "Preds: [('PROPN', \"Nevruz'u\"), ('DET', 'her'), ('NOUN', 'yıl'), ('ADJ', 'Türk'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PROPN', 'Ahmet'), ('NOUN', '<UNK>'), ('NOUN', 'dünyası'), ('PUNCT', ','), ('NOUN', '<UNK>'), ('NOUN', 'dünyasıyla'), ('NOUN', 'el'), ('NOUN', 'ele'), ('VERB', 'verecek'), ('PUNCT', ','), ('PRON', 'oradan'), ('PROPN', '<UNK>'), ('PUNCT', ','), ('PRON', 'oradan'), ('NOUN', 'Emir'), ('NOUN', '<UNK>'), ('PUNCT', ','), ('PROPN', '<UNK>'), ('NOUN', '<UNK>'), ('PUNCT', ','), ('NOUN', 'Gül'), ('NOUN', 'Babaları'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:24:33: step 1720/50000, loss = 0.237698 (0.030 sec/batch)\n",
            "2021-04-20 20:24:35: step 1740/50000, loss = 0.302850 (0.033 sec/batch)\n",
            "2021-04-20 20:24:36: step 1760/50000, loss = 0.250193 (0.033 sec/batch)\n",
            "2021-04-20 20:24:37: step 1780/50000, loss = 0.232463 (0.034 sec/batch)\n",
            "2021-04-20 20:24:38: step 1800/50000, loss = 0.281138 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1800: train_loss = 0.250223, dev_score = 0.917365\n",
            "Preds: [('NOUN', '<UNK>'), ('CCONJ', 've'), ('NOUN', 'arkadaşlarını'), ('VERB', '<UNK>'), ('VERB', 'severler'), ('PUNCT', '.')]\n",
            "2021-04-20 20:24:40: step 1820/50000, loss = 0.242577 (0.028 sec/batch)\n",
            "2021-04-20 20:24:42: step 1840/50000, loss = 0.248490 (0.030 sec/batch)\n",
            "2021-04-20 20:24:43: step 1860/50000, loss = 0.253563 (0.034 sec/batch)\n",
            "2021-04-20 20:24:44: step 1880/50000, loss = 0.222241 (0.029 sec/batch)\n",
            "2021-04-20 20:24:45: step 1900/50000, loss = 0.219508 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1900: train_loss = 0.239716, dev_score = 0.915886\n",
            "Preds: [('DET', 'Bu'), ('PUNCT', '\"'), ('VERB', '<UNK>'), ('PUNCT', '\"'), ('NOUN', 'süreci'), ('PUNCT', ','), ('ADV', 'özellikle'), ('NUM', '<UNK>'), ('NOUN', 'yıllardan'), ('ADP', 'sonra'), ('ADV', 'çok'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:24:47: step 1920/50000, loss = 0.226883 (0.035 sec/batch)\n",
            "2021-04-20 20:24:49: step 1940/50000, loss = 0.279196 (0.035 sec/batch)\n",
            "2021-04-20 20:24:50: step 1960/50000, loss = 0.210895 (0.025 sec/batch)\n",
            "2021-04-20 20:24:51: step 1980/50000, loss = 0.217871 (0.032 sec/batch)\n",
            "2021-04-20 20:24:52: step 2000/50000, loss = 0.257748 (0.039 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2000: train_loss = 0.234616, dev_score = 0.916297\n",
            "Preds: [('NOUN', '<UNK>'), ('PROPN', '<UNK>'), ('ADP', 'karşı'), ('NOUN', 'harekete'), ('VERB', 'geçti')]\n",
            "2021-04-20 20:24:54: step 2020/50000, loss = 0.212168 (0.028 sec/batch)\n",
            "2021-04-20 20:24:56: step 2040/50000, loss = 0.262643 (0.035 sec/batch)\n",
            "2021-04-20 20:24:57: step 2060/50000, loss = 0.248599 (0.029 sec/batch)\n",
            "2021-04-20 20:24:58: step 2080/50000, loss = 0.244336 (0.028 sec/batch)\n",
            "2021-04-20 20:24:59: step 2100/50000, loss = 0.309689 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2100: train_loss = 0.229064, dev_score = 0.920158\n",
            "Preds: [('PROPN', 'Ankara'), ('PUNCT', ','), ('NOUN', '<UNK>'), ('VERB', 'konuşması'), ('NOUN', '<UNK>'), ('NOUN', 'gösterisine'), ('VERB', 'dönen'), ('NOUN', 'KKTC'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:25:02: step 2120/50000, loss = 0.156485 (0.026 sec/batch)\n",
            "2021-04-20 20:25:03: step 2140/50000, loss = 0.231711 (0.033 sec/batch)\n",
            "2021-04-20 20:25:04: step 2160/50000, loss = 0.233436 (0.028 sec/batch)\n",
            "2021-04-20 20:25:05: step 2180/50000, loss = 0.234777 (0.032 sec/batch)\n",
            "2021-04-20 20:25:06: step 2200/50000, loss = 0.206257 (0.035 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2200: train_loss = 0.220975, dev_score = 0.919172\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:25:09: step 2220/50000, loss = 0.215719 (0.028 sec/batch)\n",
            "2021-04-20 20:25:10: step 2240/50000, loss = 0.243404 (0.035 sec/batch)\n",
            "2021-04-20 20:25:11: step 2260/50000, loss = 0.242642 (0.030 sec/batch)\n",
            "2021-04-20 20:25:12: step 2280/50000, loss = 0.232177 (0.029 sec/batch)\n",
            "2021-04-20 20:25:13: step 2300/50000, loss = 0.214136 (0.026 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2300: train_loss = 0.211513, dev_score = 0.919336\n",
            "Preds: [('ADJ', 'Geçen'), ('NOUN', 'hafta'), ('DET', 'bu'), ('NOUN', 'köşede'), ('PUNCT', ','), ('ADV', 'doğrudan'), ('NUM', '2.')]\n",
            "2021-04-20 20:25:16: step 2320/50000, loss = 0.199936 (0.029 sec/batch)\n",
            "2021-04-20 20:25:17: step 2340/50000, loss = 0.182744 (0.029 sec/batch)\n",
            "2021-04-20 20:25:18: step 2360/50000, loss = 0.214971 (0.032 sec/batch)\n",
            "2021-04-20 20:25:19: step 2380/50000, loss = 0.225627 (0.029 sec/batch)\n",
            "2021-04-20 20:25:20: step 2400/50000, loss = 0.207643 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2400: train_loss = 0.205386, dev_score = 0.921472\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', 'konuşmasında'), ('DET', 'bu'), ('NOUN', 'yıldan'), ('ADP', 'itibaren'), ('PUNCT', '\"'), ('NOUN', '<UNK>'), ('PROPN', '<UNK>'), ('ADJ', 'Uluslararası'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('NOUN', 'kararı'), ('VERB', 'aldıklarını'), ('VERB', 'söyledi'), ('PUNCT', '.')]\n",
            "2021-04-20 20:25:23: step 2420/50000, loss = 0.239575 (0.027 sec/batch)\n",
            "2021-04-20 20:25:24: step 2440/50000, loss = 0.174807 (0.031 sec/batch)\n",
            "2021-04-20 20:25:25: step 2460/50000, loss = 0.178557 (0.027 sec/batch)\n",
            "2021-04-20 20:25:26: step 2480/50000, loss = 0.233321 (0.037 sec/batch)\n",
            "2021-04-20 20:25:27: step 2500/50000, loss = 0.187468 (0.035 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2500: train_loss = 0.203526, dev_score = 0.919829\n",
            "Preds: [('VERB', '<UNK>'), ('CCONJ', 'eğer'), ('DET', 'bir'), ('NOUN', '<UNK>'), ('PUNCT', ','), ('PRON', 'kendi'), ('NOUN', '<UNK>'), ('ADJ', 'yanında'), ('ADJ', 'diğer'), ('NOUN', 'teknedeki'), ('NOUN', 'insanların'), ('CCONJ', 'da'), ('NOUN', 'güvenliği'), ('VERB', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', ','), ('VERB', 'çatışma'), ('NOUN', 'sonrası'), ('NOUN', 'karşılıklı'), ('NOUN', 'yardım'), ('VERB', '<UNK>'), ('NOUN', 'pozisyonda'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:25:30: step 2520/50000, loss = 0.190389 (0.032 sec/batch)\n",
            "2021-04-20 20:25:31: step 2540/50000, loss = 0.212638 (0.036 sec/batch)\n",
            "2021-04-20 20:25:32: step 2560/50000, loss = 0.254110 (0.028 sec/batch)\n",
            "2021-04-20 20:25:33: step 2580/50000, loss = 0.263575 (0.048 sec/batch)\n",
            "2021-04-20 20:25:35: step 2600/50000, loss = 0.183326 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2600: train_loss = 0.192984, dev_score = 0.920651\n",
            "Preds: [('ADV', 'En'), ('ADJ', 'son'), ('VERB', 'gittiğim'), ('NOUN', '<UNK>'), ('ADV', 'mesela'), ('PUNCT', '.')]\n",
            "2021-04-20 20:25:37: step 2620/50000, loss = 0.224605 (0.031 sec/batch)\n",
            "2021-04-20 20:25:38: step 2640/50000, loss = 0.164988 (0.030 sec/batch)\n",
            "2021-04-20 20:25:39: step 2660/50000, loss = 0.183452 (0.028 sec/batch)\n",
            "2021-04-20 20:25:40: step 2680/50000, loss = 0.194476 (0.038 sec/batch)\n",
            "2021-04-20 20:25:42: step 2700/50000, loss = 0.199206 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2700: train_loss = 0.194365, dev_score = 0.921554\n",
            "Preds: [('PRON', 'Burada'), ('VERB', 'toplanan'), ('NOUN', '<UNK>'), ('NOUN', 'duyarlılığını'), ('VERB', '<UNK>'), ('PUNCT', '...')]\n",
            "2021-04-20 20:25:44: step 2720/50000, loss = 0.201413 (0.041 sec/batch)\n",
            "2021-04-20 20:25:45: step 2740/50000, loss = 0.210218 (0.033 sec/batch)\n",
            "2021-04-20 20:25:46: step 2760/50000, loss = 0.203427 (0.032 sec/batch)\n",
            "2021-04-20 20:25:47: step 2780/50000, loss = 0.176765 (0.036 sec/batch)\n",
            "2021-04-20 20:25:49: step 2800/50000, loss = 0.173578 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2800: train_loss = 0.184015, dev_score = 0.921554\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', 'verme'), ('ADJ', 'amaçlı'), ('VERB', '<UNK>'), ('PUNCT', ','), ('ADV', 'daha'), ('ADJ', 'uzun'), ('DET', 'bir'), ('NOUN', 'süre'), ('ADJ', 'hızlı'), ('NOUN', 'yürüyüş'), ('NOUN', '<UNK>'), ('VERB', 'yapılması'), ('NOUN', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:25:51: step 2820/50000, loss = 0.166100 (0.026 sec/batch)\n",
            "2021-04-20 20:25:52: step 2840/50000, loss = 0.162214 (0.027 sec/batch)\n",
            "2021-04-20 20:25:53: step 2860/50000, loss = 0.154082 (0.029 sec/batch)\n",
            "2021-04-20 20:25:54: step 2880/50000, loss = 0.199146 (0.027 sec/batch)\n",
            "2021-04-20 20:25:56: step 2900/50000, loss = 0.177514 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2900: train_loss = 0.178702, dev_score = 0.923526\n",
            "Preds: [('DET', 'Bu'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('ADV', 'biraz'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:25:58: step 2920/50000, loss = 0.174170 (0.035 sec/batch)\n",
            "2021-04-20 20:25:59: step 2940/50000, loss = 0.162168 (0.034 sec/batch)\n",
            "2021-04-20 20:26:00: step 2960/50000, loss = 0.170649 (0.034 sec/batch)\n",
            "2021-04-20 20:26:01: step 2980/50000, loss = 0.173132 (0.029 sec/batch)\n",
            "2021-04-20 20:26:02: step 3000/50000, loss = 0.157434 (0.035 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3000: train_loss = 0.174199, dev_score = 0.920733\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', 'sınıfta'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:26:05: step 3020/50000, loss = 0.153018 (0.037 sec/batch)\n",
            "2021-04-20 20:26:06: step 3040/50000, loss = 0.165776 (0.033 sec/batch)\n",
            "2021-04-20 20:26:07: step 3060/50000, loss = 0.164040 (0.034 sec/batch)\n",
            "2021-04-20 20:26:08: step 3080/50000, loss = 0.155273 (0.031 sec/batch)\n",
            "2021-04-20 20:26:10: step 3100/50000, loss = 0.166973 (0.052 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3100: train_loss = 0.170439, dev_score = 0.922047\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', 'ikram'), ('VERB', 'etmek'), ('ADP', 'için'), ('PUNCT', '...')]\n",
            "2021-04-20 20:26:12: step 3120/50000, loss = 0.168444 (0.029 sec/batch)\n",
            "2021-04-20 20:26:13: step 3140/50000, loss = 0.186291 (0.032 sec/batch)\n",
            "2021-04-20 20:26:14: step 3160/50000, loss = 0.204337 (0.034 sec/batch)\n",
            "2021-04-20 20:26:15: step 3180/50000, loss = 0.172578 (0.033 sec/batch)\n",
            "2021-04-20 20:26:17: step 3200/50000, loss = 0.124538 (0.035 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3200: train_loss = 0.169076, dev_score = 0.921390\n",
            "Preds: [('NUM', 'Yedi'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:26:19: step 3220/50000, loss = 0.172093 (0.027 sec/batch)\n",
            "2021-04-20 20:26:20: step 3240/50000, loss = 0.154078 (0.026 sec/batch)\n",
            "2021-04-20 20:26:21: step 3260/50000, loss = 0.133477 (0.030 sec/batch)\n",
            "2021-04-20 20:26:22: step 3280/50000, loss = 0.162701 (0.035 sec/batch)\n",
            "2021-04-20 20:26:24: step 3300/50000, loss = 0.172766 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3300: train_loss = 0.161365, dev_score = 0.921636\n",
            "Preds: [('ADV', 'Sadece'), ('DET', 'şu'), ('NOUN', 'kadarını'), ('VERB', '<UNK>'), ('PUNCT', ';'), ('DET', 'her'), ('NOUN', 'noktada'), ('CCONJ', 've'), ('DET', 'her'), ('NOUN', 'yönde'), ('ADJ', 'aynı'), ('VERB', '<UNK>'), ('ADP', 'gibi'), ('DET', 'bir'), ('NOUN', '<UNK>'), ('VERB', 'taşıyan'), ('ADJ', 'gerçek'), ('DET', 'bir'), ('NOUN', 'uzay'), ('NOUN', 'zamanı'), ('VERB', 'bulma'), ('NOUN', '<UNK>'), ('VERB', 'hesaplamak'), ('ADP', 'için'), ('PUNCT', ','), ('DET', 'o'), ('NOUN', 'özelliği'), ('VERB', 'taşıyan'), ('NOUN', '<UNK>'), ('NOUN', 'tümüne'), ('ADP', 'ilişkin'), ('NOUN', '<UNK>'), ('VERB', 'topladık'), ('PUNCT', '.')]\n",
            "2021-04-20 20:26:26: step 3320/50000, loss = 0.132790 (0.033 sec/batch)\n",
            "2021-04-20 20:26:27: step 3340/50000, loss = 0.162561 (0.026 sec/batch)\n",
            "2021-04-20 20:26:28: step 3360/50000, loss = 0.196572 (0.035 sec/batch)\n",
            "2021-04-20 20:26:29: step 3380/50000, loss = 0.194180 (0.033 sec/batch)\n",
            "2021-04-20 20:26:31: step 3400/50000, loss = 0.185212 (0.034 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3400: train_loss = 0.154768, dev_score = 0.923115\n",
            "Preds: [('CCONJ', 'Eğer'), ('PRON', 'beni'), ('NOUN', '<UNK>'), ('DET', 'bütün'), ('PRON', 'bunları'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:26:33: step 3420/50000, loss = 0.177374 (0.033 sec/batch)\n",
            "2021-04-20 20:26:34: step 3440/50000, loss = 0.134334 (0.029 sec/batch)\n",
            "2021-04-20 20:26:35: step 3460/50000, loss = 0.132103 (0.031 sec/batch)\n",
            "2021-04-20 20:26:36: step 3480/50000, loss = 0.110242 (0.030 sec/batch)\n",
            "2021-04-20 20:26:38: step 3500/50000, loss = 0.113632 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3500: train_loss = 0.152272, dev_score = 0.922293\n",
            "Preds: [('NOUN', '<UNK>'), ('PUNCT', ','), ('NOUN', 'spor'), ('NOUN', '<UNK>'), ('ADV', 'çok'), ('ADJ', 'özel'), ('DET', 'bir'), ('NOUN', 'insanı'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:26:40: step 3520/50000, loss = 0.160969 (0.034 sec/batch)\n",
            "2021-04-20 20:26:41: step 3540/50000, loss = 0.120724 (0.028 sec/batch)\n",
            "2021-04-20 20:26:42: step 3560/50000, loss = 0.168850 (0.027 sec/batch)\n",
            "2021-04-20 20:26:44: step 3580/50000, loss = 0.171566 (0.033 sec/batch)\n",
            "2021-04-20 20:26:45: step 3600/50000, loss = 0.132801 (0.040 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3600: train_loss = 0.147425, dev_score = 0.923854\n",
            "Preds: [('ADV', 'Sadece'), ('NUM', 'yedi'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', 'haberdar'), ('VERB', '<UNK>'), ('PUNCT', ','), ('NOUN', 'köye'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:26:47: step 3620/50000, loss = 0.166008 (0.034 sec/batch)\n",
            "2021-04-20 20:26:48: step 3640/50000, loss = 0.126802 (0.035 sec/batch)\n",
            "2021-04-20 20:26:49: step 3660/50000, loss = 0.165066 (0.027 sec/batch)\n",
            "2021-04-20 20:26:51: step 3680/50000, loss = 0.141767 (0.031 sec/batch)\n",
            "2021-04-20 20:26:52: step 3700/50000, loss = 0.115526 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3700: train_loss = 0.146172, dev_score = 0.921390\n",
            "Preds: [('ADV', 'Sürekli'), ('NOUN', 'aklımda'), ('VERB', 'Selim'), ('CCONJ', 've'), ('PRON', 'onun'), ('ADJ', 'güzel'), ('NOUN', '<UNK>'), ('VERB', 'vardı'), ('PUNCT', '.')]\n",
            "2021-04-20 20:26:54: step 3720/50000, loss = 0.168163 (0.029 sec/batch)\n",
            "2021-04-20 20:26:55: step 3740/50000, loss = 0.135963 (0.032 sec/batch)\n",
            "2021-04-20 20:26:56: step 3760/50000, loss = 0.185908 (0.032 sec/batch)\n",
            "2021-04-20 20:26:58: step 3780/50000, loss = 0.114924 (0.027 sec/batch)\n",
            "2021-04-20 20:26:59: step 3800/50000, loss = 0.140568 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3800: train_loss = 0.144361, dev_score = 0.922951\n",
            "Preds: [('DET', 'Bu'), ('NOUN', 'buluşun'), ('PUNCT', ','), ('NOUN', 'gelecekte'), ('PUNCT', ','), ('NOUN', '<UNK>'), ('NOUN', 'üretimi'), ('PUNCT', ','), ('ADJ', 'küresel'), ('NOUN', '<UNK>'), ('NOUN', 'mücadele'), ('CCONJ', 've'), ('NOUN', 'ilaç'), ('NOUN', 'üretiminde'), ('NOUN', 'insanlığa'), ('ADJ', 'büyük'), ('NOUN', 'faydası'), ('VERB', 'olPoteceği'), ('VERB', 'belirtiliyor'), ('PUNCT', '.')]\n",
            "2021-04-20 20:27:01: step 3820/50000, loss = 0.142432 (0.033 sec/batch)\n",
            "2021-04-20 20:27:02: step 3840/50000, loss = 0.136678 (0.035 sec/batch)\n",
            "2021-04-20 20:27:03: step 3860/50000, loss = 0.135985 (0.037 sec/batch)\n",
            "2021-04-20 20:27:05: step 3880/50000, loss = 0.138444 (0.036 sec/batch)\n",
            "2021-04-20 20:27:06: step 3900/50000, loss = 0.153596 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3900: train_loss = 0.141435, dev_score = 0.920158\n",
            "Preds: [('ADJ', '<UNK>'), ('VERB', 'geçip'), ('VERB', '<UNK>'), ('PUNCT', ','), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('ADJ', 'arasında'), ('NOUN', '<UNK>'), ('NOUN', 'yol'), ('VERB', '<UNK>'), ('NOUN', 'köye'), ('VERB', 'ulaştık'), ('PUNCT', '.')]\n",
            "2021-04-20 20:27:08: step 3920/50000, loss = 0.189147 (0.034 sec/batch)\n",
            "2021-04-20 20:27:09: step 3940/50000, loss = 0.109337 (0.033 sec/batch)\n",
            "2021-04-20 20:27:10: step 3960/50000, loss = 0.153513 (0.052 sec/batch)\n",
            "2021-04-20 20:27:12: step 3980/50000, loss = 0.188617 (0.029 sec/batch)\n",
            "2021-04-20 20:27:13: step 4000/50000, loss = 0.153110 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4000: train_loss = 0.134691, dev_score = 0.921143\n",
            "Preds: [('PROPN', '<UNK>'), ('ADJ', 'saldırı'), ('NOUN', 'emri')]\n",
            "2021-04-20 20:27:15: step 4020/50000, loss = 0.160986 (0.030 sec/batch)\n",
            "2021-04-20 20:27:16: step 4040/50000, loss = 0.112347 (0.031 sec/batch)\n",
            "2021-04-20 20:27:17: step 4060/50000, loss = 0.178316 (0.029 sec/batch)\n",
            "2021-04-20 20:27:19: step 4080/50000, loss = 0.129617 (0.026 sec/batch)\n",
            "2021-04-20 20:27:20: step 4100/50000, loss = 0.121233 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4100: train_loss = 0.130189, dev_score = 0.919993\n",
            "Preds: [('PRON', 'Onlar'), ('VERB', 'bakıyorlar'), ('PUNCT', ','), ('NOUN', 'gözlerini'), ('PRON', 'bana'), ('VERB', '<UNK>')]\n",
            "2021-04-20 20:27:22: step 4120/50000, loss = 0.136449 (0.035 sec/batch)\n",
            "2021-04-20 20:27:23: step 4140/50000, loss = 0.123142 (0.029 sec/batch)\n",
            "2021-04-20 20:27:24: step 4160/50000, loss = 0.101527 (0.028 sec/batch)\n",
            "2021-04-20 20:27:26: step 4180/50000, loss = 0.141683 (0.034 sec/batch)\n",
            "2021-04-20 20:27:27: step 4200/50000, loss = 0.167678 (0.052 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4200: train_loss = 0.129252, dev_score = 0.922211\n",
            "Preds: [('NOUN', 'Baba'), ('NOUN', 'sevgisinden'), ('NOUN', '<UNK>'), ('VERB', 'büyüyen'), ('PROPN', '<UNK>'), ('PUNCT', ','), ('PRON', 'kendi'), ('NOUN', 'çocuklarına'), ('NOUN', 'düşkünlüğünü'), ('PUNCT', ','), ('NOUN', 'aileyi'), ('ADV', 'çok'), ('ADV', 'iyi'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:27:29: step 4220/50000, loss = 0.141879 (0.032 sec/batch)\n",
            "2021-04-20 20:27:30: step 4240/50000, loss = 0.099435 (0.031 sec/batch)\n",
            "2021-04-20 20:27:32: step 4260/50000, loss = 0.131315 (0.033 sec/batch)\n",
            "2021-04-20 20:27:33: step 4280/50000, loss = 0.115231 (0.037 sec/batch)\n",
            "2021-04-20 20:27:34: step 4300/50000, loss = 0.157986 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4300: train_loss = 0.125042, dev_score = 0.921061\n",
            "Preds: [('DET', 'Bu'), ('NOUN', 'paraların'), ('DET', 'bir'), ('NOUN', 'yıldır'), ('NOUN', '<UNK>'), ('ADV', 'bile'), ('VERB', '<UNK>'), ('VERB', 'biliyoruz'), ('PUNCT', '.')]\n",
            "2021-04-20 20:27:36: step 4320/50000, loss = 0.103943 (0.033 sec/batch)\n",
            "2021-04-20 20:27:38: step 4340/50000, loss = 0.188299 (0.031 sec/batch)\n",
            "2021-04-20 20:27:39: step 4360/50000, loss = 0.118060 (0.033 sec/batch)\n",
            "2021-04-20 20:27:40: step 4380/50000, loss = 0.109144 (0.037 sec/batch)\n",
            "2021-04-20 20:27:41: step 4400/50000, loss = 0.119939 (0.040 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4400: train_loss = 0.127638, dev_score = 0.922129\n",
            "Preds: [('NOUN', 'Yazık'), ('ADP', 'ki'), ('NOUN', 'toplum'), ('ADP', 'olarak'), ('PRON', 'bunun'), ('NOUN', 'farkında'), ('AUX', 'değiliz'), ('PUNCT', '.')]\n",
            "2021-04-20 20:27:43: step 4420/50000, loss = 0.132633 (0.030 sec/batch)\n",
            "2021-04-20 20:27:45: step 4440/50000, loss = 0.128430 (0.036 sec/batch)\n",
            "2021-04-20 20:27:46: step 4460/50000, loss = 0.144535 (0.033 sec/batch)\n",
            "2021-04-20 20:27:47: step 4480/50000, loss = 0.125115 (0.033 sec/batch)\n",
            "2021-04-20 20:27:48: step 4500/50000, loss = 0.101406 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4500: train_loss = 0.118943, dev_score = 0.922376\n",
            "Preds: [('VERB', 'Yaptığı'), ('DET', 'tüm'), ('NOUN', 'hareketler'), ('PUNCT', ','), ('PUNCT', '\"'), ('PRON', 'ne'), ('VERB', '<UNK>'), ('CCONJ', 'da'), ('DET', 'bir'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '\"'), ('_', '<UNK>'), ('NOUN', 'şeklinde'), ('AUX', 'ydi'), ('PUNCT', '.')]\n",
            "2021-04-20 20:27:50: step 4520/50000, loss = 0.110859 (0.034 sec/batch)\n",
            "2021-04-20 20:27:52: step 4540/50000, loss = 0.134004 (0.029 sec/batch)\n",
            "2021-04-20 20:27:53: step 4560/50000, loss = 0.119937 (0.028 sec/batch)\n",
            "2021-04-20 20:27:54: step 4580/50000, loss = 0.119362 (0.036 sec/batch)\n",
            "2021-04-20 20:27:55: step 4600/50000, loss = 0.128185 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4600: train_loss = 0.117676, dev_score = 0.921718\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('NOUN', '<UNK>'), ('ADV', 'sert'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:27:58: step 4620/50000, loss = 0.101360 (0.033 sec/batch)\n",
            "2021-04-20 20:27:59: step 4640/50000, loss = 0.095146 (0.033 sec/batch)\n",
            "2021-04-20 20:28:00: step 4660/50000, loss = 0.116540 (0.029 sec/batch)\n",
            "2021-04-20 20:28:02: step 4680/50000, loss = 0.121083 (0.035 sec/batch)\n",
            "2021-04-20 20:28:03: step 4700/50000, loss = 0.118929 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4700: train_loss = 0.113786, dev_score = 0.923690\n",
            "Preds: [('NUM', '24'), ('PROPN', \"Ocak'ta\"), ('PROPN', \"İstanbul'daki\"), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('ADJ', 'başarısız'), ('VERB', 'olan'), ('CCONJ', 've'), ('PROPN', '<UNK>'), ('VERB', '<UNK>'), ('PROPN', 'Metin'), ('PROPN', 'Aydoğan'), ('PUNCT', ','), ('PROPN', \"FIFA'nın\"), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', 'başarıyla'), ('VERB', 'tamamladı'), ('PUNCT', '.')]\n",
            "2021-04-20 20:28:05: step 4720/50000, loss = 0.142008 (0.030 sec/batch)\n",
            "2021-04-20 20:28:06: step 4740/50000, loss = 0.113707 (0.028 sec/batch)\n",
            "2021-04-20 20:28:07: step 4760/50000, loss = 0.084411 (0.031 sec/batch)\n",
            "2021-04-20 20:28:09: step 4780/50000, loss = 0.116679 (0.034 sec/batch)\n",
            "2021-04-20 20:28:10: step 4800/50000, loss = 0.134399 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4800: train_loss = 0.115133, dev_score = 0.921883\n",
            "Preds: [('PROPN', '<UNK>'), ('VERB', '<UNK>')]\n",
            "2021-04-20 20:28:12: step 4820/50000, loss = 0.102905 (0.027 sec/batch)\n",
            "2021-04-20 20:28:13: step 4840/50000, loss = 0.117610 (0.036 sec/batch)\n",
            "2021-04-20 20:28:14: step 4860/50000, loss = 0.115918 (0.028 sec/batch)\n",
            "2021-04-20 20:28:16: step 4880/50000, loss = 0.103800 (0.028 sec/batch)\n",
            "2021-04-20 20:28:17: step 4900/50000, loss = 0.097164 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4900: train_loss = 0.110086, dev_score = 0.922129\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:28:19: step 4920/50000, loss = 0.118079 (0.027 sec/batch)\n",
            "2021-04-20 20:28:20: step 4940/50000, loss = 0.090163 (0.032 sec/batch)\n",
            "2021-04-20 20:28:21: step 4960/50000, loss = 0.068082 (0.030 sec/batch)\n",
            "2021-04-20 20:28:23: step 4980/50000, loss = 0.086035 (0.032 sec/batch)\n",
            "2021-04-20 20:28:24: step 5000/50000, loss = 0.091750 (0.034 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5000: train_loss = 0.105971, dev_score = 0.922622\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', 'bölgelere'), ('NOUN', 'has'), ('ADJ', 'yiyecek'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('CCONJ', 'de'), ('NOUN', 'ikram'), ('VERB', 'edildi'), ('PUNCT', '.')]\n",
            "2021-04-20 20:28:26: step 5020/50000, loss = 0.072547 (0.034 sec/batch)\n",
            "2021-04-20 20:28:27: step 5040/50000, loss = 0.107672 (0.030 sec/batch)\n",
            "2021-04-20 20:28:29: step 5060/50000, loss = 0.093903 (0.029 sec/batch)\n",
            "2021-04-20 20:28:30: step 5080/50000, loss = 0.136139 (0.029 sec/batch)\n",
            "2021-04-20 20:28:31: step 5100/50000, loss = 0.095812 (0.034 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5100: train_loss = 0.104502, dev_score = 0.920979\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', 'yardım'), ('NOUN', 'et'), ('PUNCT', '.')]\n",
            "2021-04-20 20:28:33: step 5120/50000, loss = 0.087996 (0.026 sec/batch)\n",
            "2021-04-20 20:28:34: step 5140/50000, loss = 0.097361 (0.032 sec/batch)\n",
            "2021-04-20 20:28:36: step 5160/50000, loss = 0.105735 (0.029 sec/batch)\n",
            "2021-04-20 20:28:37: step 5180/50000, loss = 0.088701 (0.037 sec/batch)\n",
            "2021-04-20 20:28:38: step 5200/50000, loss = 0.102204 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5200: train_loss = 0.102820, dev_score = 0.921554\n",
            "Preds: [('VERB', '<UNK>'), ('VERB', 'soyunma'), ('NOUN', 'odasına'), ('PUNCT', '.')]\n",
            "2021-04-20 20:28:40: step 5220/50000, loss = 0.100588 (0.037 sec/batch)\n",
            "2021-04-20 20:28:42: step 5240/50000, loss = 0.078691 (0.032 sec/batch)\n",
            "2021-04-20 20:28:43: step 5260/50000, loss = 0.086845 (0.029 sec/batch)\n",
            "2021-04-20 20:28:44: step 5280/50000, loss = 0.097699 (0.028 sec/batch)\n",
            "2021-04-20 20:28:45: step 5300/50000, loss = 0.108225 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5300: train_loss = 0.101970, dev_score = 0.922293\n",
            "Preds: [('ADV', 'Hele'), ('PRON', 'bizim'), ('ADP', 'gibi'), ('NOUN', 'topluma'), ('NOUN', 'örnek'), ('VERB', 'olması'), ('VERB', 'gereken'), ('NOUN', 'futbolcuların'), ('ADJ', '<UNK>'), ('VERB', 'olmaları'), ('DET', 'çok'), ('NOUN', 'önem'), ('NOUN', 'arz'), ('VERB', 'ediyor'), ('PUNCT', '.')]\n",
            "2021-04-20 20:28:47: step 5320/50000, loss = 0.095419 (0.032 sec/batch)\n",
            "2021-04-20 20:28:49: step 5340/50000, loss = 0.118661 (0.036 sec/batch)\n",
            "2021-04-20 20:28:50: step 5360/50000, loss = 0.115527 (0.032 sec/batch)\n",
            "2021-04-20 20:28:51: step 5380/50000, loss = 0.095207 (0.038 sec/batch)\n",
            "2021-04-20 20:28:52: step 5400/50000, loss = 0.107969 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5400: train_loss = 0.100736, dev_score = 0.922211\n",
            "Preds: [('PRON', 'O'), ('PUNCT', ','), ('ADJ', 'kişisel'), ('CCONJ', 've'), ('ADJ', 'günlük'), ('NOUN', 'siyaset'), ('NOUN', 'yerine'), ('DET', 'her'), ('NOUN', 'zaman'), ('ADJ', 'uzun'), ('_', 'vadeli'), ('NOUN', '<UNK>'), ('ADP', 'li'), ('VERB', 'düşünen'), ('PUNCT', ','), ('NOUN', '<UNK>'), ('NOUN', 'halkın'), ('NOUN', 'yaşamını'), ('VERB', 'iyileştirmek'), ('PUNCT', ','), ('_', 'adaletli'), ('NOUN', 'adalet'), ('ADP', 'li'), ('DET', 'bir'), ('NOUN', 'toplum'), ('CCONJ', 've'), ('NOUN', 'dünya'), ('VERB', 'yaratmak'), ('ADP', 'olarak'), ('VERB', 'anlayan'), ('DET', 'bir'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:28:54: step 5420/50000, loss = 0.118184 (0.031 sec/batch)\n",
            "2021-04-20 20:28:56: step 5440/50000, loss = 0.107743 (0.031 sec/batch)\n",
            "2021-04-20 20:28:57: step 5460/50000, loss = 0.072778 (0.038 sec/batch)\n",
            "2021-04-20 20:28:58: step 5480/50000, loss = 0.084353 (0.029 sec/batch)\n",
            "2021-04-20 20:28:59: step 5500/50000, loss = 0.088623 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5500: train_loss = 0.098877, dev_score = 0.922293\n",
            "Preds: [('CCONJ', 'Eğer'), ('ADV', 'böyle'), ('VERB', 'olursa'), ('PUNCT', ','), ('NOUN', 'bilimin'), ('NOUN', 'kapısına'), ('NOUN', 'kilit'), ('VERB', 'vurulur'), ('PUNCT', '.')]\n",
            "2021-04-20 20:29:02: step 5520/50000, loss = 0.101067 (0.037 sec/batch)\n",
            "2021-04-20 20:29:03: step 5540/50000, loss = 0.100501 (0.030 sec/batch)\n",
            "2021-04-20 20:29:04: step 5560/50000, loss = 0.091224 (0.031 sec/batch)\n",
            "2021-04-20 20:29:05: step 5580/50000, loss = 0.093887 (0.026 sec/batch)\n",
            "2021-04-20 20:29:06: step 5600/50000, loss = 0.087054 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5600: train_loss = 0.096229, dev_score = 0.922047\n",
            "Preds: [('DET', 'Her'), ('NOUN', 'söz'), ('PUNCT', ','), ('DET', 'her'), ('NOUN', 'davranış'), ('PUNCT', ','), ('DET', 'her'), ('NOUN', 'bakış'), ('PRON', 'beni'), ('VERB', '<UNK>'), ('PUNCT', ','), ('NOUN', 'yüreğimi'), ('ADJ', 'acılarla'), ('PUNCT', ','), ('NOUN', '<UNK>'), ('VERB', 'dolduruyor'), ('PUNCT', ','), ('ADV', 'sonra'), ('CCONJ', 'da'), ('PRON', 'bunları'), ('NOUN', 'isyan'), ('NOUN', '<UNK>'), ('NOUN', 'dışarıya'), ('VERB', 'atmaya'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:29:09: step 5620/50000, loss = 0.107170 (0.036 sec/batch)\n",
            "2021-04-20 20:29:10: step 5640/50000, loss = 0.111120 (0.032 sec/batch)\n",
            "2021-04-20 20:29:11: step 5660/50000, loss = 0.129490 (0.031 sec/batch)\n",
            "2021-04-20 20:29:12: step 5680/50000, loss = 0.095537 (0.029 sec/batch)\n",
            "2021-04-20 20:29:13: step 5700/50000, loss = 0.105399 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5700: train_loss = 0.094390, dev_score = 0.924265\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', 'ilgilendiren'), ('NOUN', 'perakende'), ('NOUN', '<UNK>'), ('VERB', 'bulunurken'), ('PUNCT', ','), ('PROPN', '<UNK>'), ('VERB', 'belirlediği'), ('NOUN', 'çıkış'), ('NOUN', 'fiyatına'), ('NOUN', 'ÖTV'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('ADJ', 'yanı'), ('NOUN', 'sıra'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', 'hizmet'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('PUNCT', ','), ('NOUN', 'sanayi'), ('ADP', 'için'), ('VERB', 'kullanılacak'), ('NOUN', 'doğalgazda'), ('AUX', 'ise'), ('NOUN', 'boru'), ('VERB', 'hattı'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:29:16: step 5720/50000, loss = 0.092988 (0.032 sec/batch)\n",
            "2021-04-20 20:29:17: step 5740/50000, loss = 0.078831 (0.029 sec/batch)\n",
            "2021-04-20 20:29:18: step 5760/50000, loss = 0.085967 (0.032 sec/batch)\n",
            "2021-04-20 20:29:19: step 5780/50000, loss = 0.082512 (0.033 sec/batch)\n",
            "2021-04-20 20:29:20: step 5800/50000, loss = 0.086899 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5800: train_loss = 0.092498, dev_score = 0.922540\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('NOUN', 'gül'), ('NOUN', 'bahçesine'), ('VERB', 'dönüşmesi'), ('PUNCT', ';'), ('NOUN', 'hal'), ('_', '<UNK>'), ('ADV', 'böyle'), ('AUX', 'yken'), ('ADJ', 'büyük'), ('NOUN', 'çaba'), ('VERB', '<UNK>'), ('PUNCT', '...')]\n",
            "2021-04-20 20:29:23: step 5820/50000, loss = 0.096402 (0.037 sec/batch)\n",
            "2021-04-20 20:29:24: step 5840/50000, loss = 0.069557 (0.030 sec/batch)\n",
            "2021-04-20 20:29:25: step 5860/50000, loss = 0.075226 (0.035 sec/batch)\n",
            "2021-04-20 20:29:26: step 5880/50000, loss = 0.109216 (0.035 sec/batch)\n",
            "2021-04-20 20:29:27: step 5900/50000, loss = 0.090153 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5900: train_loss = 0.089583, dev_score = 0.921883\n",
            "Preds: [('PROPN', '<UNK>'), ('ADV', 'çok'), ('VERB', '<UNK>'), ('PUNCT', ','), ('PRON', 'onunla'), ('VERB', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:29:30: step 5920/50000, loss = 0.075813 (0.027 sec/batch)\n",
            "2021-04-20 20:29:31: step 5940/50000, loss = 0.091344 (0.029 sec/batch)\n",
            "2021-04-20 20:29:32: step 5960/50000, loss = 0.087997 (0.033 sec/batch)\n",
            "2021-04-20 20:29:33: step 5980/50000, loss = 0.063151 (0.031 sec/batch)\n",
            "2021-04-20 20:29:35: step 6000/50000, loss = 0.112366 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6000: train_loss = 0.088812, dev_score = 0.923526\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', 'ilgilendiren'), ('ADJ', 'perakende'), ('NOUN', '<UNK>'), ('VERB', 'bulunurken'), ('PUNCT', ','), ('PROPN', '<UNK>'), ('VERB', 'belirlediği'), ('NOUN', 'çıkış'), ('NOUN', 'fiyatına'), ('NOUN', 'ÖTV'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('ADJ', 'yanı'), ('NOUN', 'sıra'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', 'hizmet'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('PUNCT', ','), ('NOUN', 'sanayi'), ('ADP', 'için'), ('VERB', 'kullanılacak'), ('NOUN', 'doğalgazda'), ('AUX', 'ise'), ('NOUN', 'boru'), ('VERB', 'hattı'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:29:37: step 6020/50000, loss = 0.082403 (0.024 sec/batch)\n",
            "2021-04-20 20:29:38: step 6040/50000, loss = 0.075587 (0.035 sec/batch)\n",
            "2021-04-20 20:29:39: step 6060/50000, loss = 0.079257 (0.029 sec/batch)\n",
            "2021-04-20 20:29:40: step 6080/50000, loss = 0.092018 (0.032 sec/batch)\n",
            "2021-04-20 20:29:42: step 6100/50000, loss = 0.075534 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6100: train_loss = 0.088434, dev_score = 0.921801\n",
            "Preds: [('ADJ', 'Gerçek'), ('VERB', 'olmayan'), ('PRON', 'birinin'), ('NOUN', '<UNK>'), ('VERB', 'girerken'), ('PUNCT', ','), ('PRON', 'onun'), ('ADP', 'gibi'), ('VERB', 'konuşurken'), ('PUNCT', ','), ('CCONJ', 'hatta'), ('PRON', 'onun'), ('ADP', 'gibi'), ('_', 'düşünürken'), ('NOUN', 'düşünür'), ('AUX', 'ken'), ('PRON', 'kendi'), ('ADJ', 'gerçek'), ('NOUN', 'kimliğine'), ('PRON', 'ne'), ('VERB', 'oluyor'), ('PUNCT', ','), ('DET', 'o'), ('ADJ', 'gerçek'), ('NOUN', 'kimlik'), ('PRON', 'nereye'), ('VERB', '<UNK>'), ('PUNCT', '?')]\n",
            "2021-04-20 20:29:44: step 6120/50000, loss = 0.094468 (0.032 sec/batch)\n",
            "2021-04-20 20:29:45: step 6140/50000, loss = 0.104011 (0.034 sec/batch)\n",
            "2021-04-20 20:29:46: step 6160/50000, loss = 0.091166 (0.030 sec/batch)\n",
            "2021-04-20 20:29:47: step 6180/50000, loss = 0.085171 (0.027 sec/batch)\n",
            "2021-04-20 20:29:49: step 6200/50000, loss = 0.084698 (0.038 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6200: train_loss = 0.084629, dev_score = 0.921061\n",
            "Preds: [('PROPN', 'Güneş'), ('ADJ', 'mavi'), ('NOUN', '<UNK>'), ('ADJ', 'ortasında'), ('AUX', 'idi'), ('PUNCT', '.')]\n",
            "2021-04-20 20:29:51: step 6220/50000, loss = 0.072731 (0.032 sec/batch)\n",
            "2021-04-20 20:29:52: step 6240/50000, loss = 0.106014 (0.031 sec/batch)\n",
            "2021-04-20 20:29:53: step 6260/50000, loss = 0.096669 (0.032 sec/batch)\n",
            "2021-04-20 20:29:54: step 6280/50000, loss = 0.078639 (0.030 sec/batch)\n",
            "2021-04-20 20:29:56: step 6300/50000, loss = 0.083889 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6300: train_loss = 0.084994, dev_score = 0.924593\n",
            "Preds: [('NUM', '3'), ('NOUN', 'adet'), ('VERB', '<UNK>'), ('NOUN', 'biber'), ('PUNCT', '('), ('ADV', 'ince'), ('VERB', 'doğranmış'), ('PUNCT', ')')]\n",
            "2021-04-20 20:29:58: step 6320/50000, loss = 0.084425 (0.028 sec/batch)\n",
            "2021-04-20 20:29:59: step 6340/50000, loss = 0.085969 (0.034 sec/batch)\n",
            "2021-04-20 20:30:00: step 6360/50000, loss = 0.089803 (0.030 sec/batch)\n",
            "2021-04-20 20:30:01: step 6380/50000, loss = 0.091428 (0.032 sec/batch)\n",
            "2021-04-20 20:30:03: step 6400/50000, loss = 0.069802 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6400: train_loss = 0.083700, dev_score = 0.923443\n",
            "Preds: [('PROPN', 'Halikarnas'), ('NOUN', '<UNK>'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('NOUN', 'Burina'), ('NOUN', '<UNK>')]\n",
            "2021-04-20 20:30:05: step 6420/50000, loss = 0.090524 (0.031 sec/batch)\n",
            "2021-04-20 20:30:06: step 6440/50000, loss = 0.067732 (0.031 sec/batch)\n",
            "2021-04-20 20:30:07: step 6460/50000, loss = 0.077942 (0.027 sec/batch)\n",
            "2021-04-20 20:30:09: step 6480/50000, loss = 0.065540 (0.030 sec/batch)\n",
            "2021-04-20 20:30:10: step 6500/50000, loss = 0.096493 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6500: train_loss = 0.080142, dev_score = 0.921965\n",
            "Preds: [('DET', 'Bir'), ('ADJ', 'yandan'), ('CCONJ', 'da'), ('VERB', '<UNK>'), ('ADJ', 'modern'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('NOUN', 'öyküler'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:30:12: step 6520/50000, loss = 0.082149 (0.036 sec/batch)\n",
            "2021-04-20 20:30:13: step 6540/50000, loss = 0.089648 (0.037 sec/batch)\n",
            "2021-04-20 20:30:14: step 6560/50000, loss = 0.073652 (0.030 sec/batch)\n",
            "2021-04-20 20:30:16: step 6580/50000, loss = 0.090318 (0.032 sec/batch)\n",
            "2021-04-20 20:30:17: step 6600/50000, loss = 0.089519 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6600: train_loss = 0.085929, dev_score = 0.922211\n",
            "Preds: [('ADJ', 'Kadın'), ('NOUN', 'çay'), ('VERB', 'getirdi'), ('PUNCT', '.')]\n",
            "2021-04-20 20:30:19: step 6620/50000, loss = 0.084256 (0.037 sec/batch)\n",
            "2021-04-20 20:30:20: step 6640/50000, loss = 0.088247 (0.030 sec/batch)\n",
            "2021-04-20 20:30:21: step 6660/50000, loss = 0.059605 (0.030 sec/batch)\n",
            "2021-04-20 20:30:23: step 6680/50000, loss = 0.077847 (0.027 sec/batch)\n",
            "2021-04-20 20:30:24: step 6700/50000, loss = 0.097462 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6700: train_loss = 0.077358, dev_score = 0.921883\n",
            "Preds: [('PROPN', '<UNK>'), ('PUNCT', ':'), ('PROPN', '<UNK>'), ('NOUN', '<UNK>'), ('ADJ', 'yok')]\n",
            "2021-04-20 20:30:26: step 6720/50000, loss = 0.065181 (0.035 sec/batch)\n",
            "2021-04-20 20:30:27: step 6740/50000, loss = 0.101426 (0.030 sec/batch)\n",
            "2021-04-20 20:30:29: step 6760/50000, loss = 0.092420 (0.026 sec/batch)\n",
            "2021-04-20 20:30:30: step 6780/50000, loss = 0.073263 (0.031 sec/batch)\n",
            "2021-04-20 20:30:31: step 6800/50000, loss = 0.055447 (0.035 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6800: train_loss = 0.076151, dev_score = 0.923936\n",
            "Preds: [('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('PUNCT', '-'), ('NOUN', '<UNK>'), ('VERB', 'oldu')]\n",
            "2021-04-20 20:30:33: step 6820/50000, loss = 0.088260 (0.032 sec/batch)\n",
            "2021-04-20 20:30:35: step 6840/50000, loss = 0.066468 (0.031 sec/batch)\n",
            "2021-04-20 20:30:36: step 6860/50000, loss = 0.072435 (0.029 sec/batch)\n",
            "2021-04-20 20:30:37: step 6880/50000, loss = 0.036573 (0.028 sec/batch)\n",
            "2021-04-20 20:30:38: step 6900/50000, loss = 0.064834 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6900: train_loss = 0.076791, dev_score = 0.921883\n",
            "Preds: [('ADJ', 'İlk'), ('ADJ', 'yüksek'), ('NOUN', '<UNK>'), ('NOUN', 'merkezleri'), ('PROPN', \"Avrupa'da\"), ('ADV', 'çok'), ('ADV', 'daha'), ('ADV', 'sonraları'), ('ADJ', 'ortaya'), ('VERB', 'çıktılar'), ('PUNCT', '.')]\n",
            "2021-04-20 20:30:40: step 6920/50000, loss = 0.100359 (0.037 sec/batch)\n",
            "2021-04-20 20:30:42: step 6940/50000, loss = 0.084399 (0.031 sec/batch)\n",
            "2021-04-20 20:30:43: step 6960/50000, loss = 0.102595 (0.036 sec/batch)\n",
            "2021-04-20 20:30:44: step 6980/50000, loss = 0.099167 (0.030 sec/batch)\n",
            "2021-04-20 20:30:45: step 7000/50000, loss = 0.075033 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7000: train_loss = 0.076736, dev_score = 0.922293\n",
            "Preds: [('ADV', 'Hızla'), ('VERB', 'şişen'), ('NOUN', '<UNK>'), ('NOUN', 'nefes'), ('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('NOUN', 'sonucu'), ('NOUN', 'kişi'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:30:48: step 7020/50000, loss = 0.064344 (0.030 sec/batch)\n",
            "2021-04-20 20:30:49: step 7040/50000, loss = 0.070994 (0.032 sec/batch)\n",
            "2021-04-20 20:30:50: step 7060/50000, loss = 0.089204 (0.033 sec/batch)\n",
            "2021-04-20 20:30:51: step 7080/50000, loss = 0.079680 (0.029 sec/batch)\n",
            "2021-04-20 20:30:52: step 7100/50000, loss = 0.087177 (0.038 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7100: train_loss = 0.075341, dev_score = 0.921883\n",
            "Preds: [('DET', 'O'), ('NOUN', 'günlere'), ('ADP', 'ait'), ('NOUN', '<UNK>'), ('NOUN', 'söz'), ('VERB', 'ederken'), ('NOUN', 'ailenin'), ('DET', 'bu'), ('NOUN', '<UNK>'), ('CCONJ', 've'), ('NOUN', '<UNK>'), ('NOUN', 'yer'), ('VERB', 'eden'), ('NOUN', 'izleri'), ('ADV', 'şöyle'), ('VERB', 'anlatır'), ('PUNCT', ','), ('PRON', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:30:55: step 7120/50000, loss = 0.083468 (0.031 sec/batch)\n",
            "2021-04-20 20:30:56: step 7140/50000, loss = 0.113340 (0.036 sec/batch)\n",
            "2021-04-20 20:30:57: step 7160/50000, loss = 0.100102 (0.037 sec/batch)\n",
            "2021-04-20 20:30:58: step 7180/50000, loss = 0.066845 (0.030 sec/batch)\n",
            "2021-04-20 20:30:59: step 7200/50000, loss = 0.045499 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7200: train_loss = 0.074660, dev_score = 0.923279\n",
            "Preds: [('ADJ', '<UNK>'), ('PRON', 'biri'), ('VERB', '<UNK>'), ('ADP', 'için'), ('PRON', 'kendi'), ('ADJ', 'gerçek'), ('NOUN', 'kimliğini'), ('NOUN', 'terk'), ('VERB', 'eden'), ('PRON', 'biri'), ('PUNCT', ','), ('NOUN', 'oyun'), ('NOUN', 'bittiğinde'), ('PRON', 'kendi'), ('NOUN', 'kimliğine'), ('VERB', '<UNK>'), ('AUX', 'mu'), ('ADV', 'hemen'), ('PUNCT', ','), ('NOUN', 'döndüğünde'), ('NOUN', 'ruhunda'), ('ADJ', 'gerçek'), ('VERB', 'olmayan'), ('PROPN', '<UNK>'), ('NOUN', 'birşeyler'), ('VERB', 'kalmıyor'), ('AUX', 'mu'), ('PUNCT', '?')]\n",
            "2021-04-20 20:31:02: step 7220/50000, loss = 0.077775 (0.030 sec/batch)\n",
            "2021-04-20 20:31:03: step 7240/50000, loss = 0.056794 (0.027 sec/batch)\n",
            "2021-04-20 20:31:04: step 7260/50000, loss = 0.055245 (0.031 sec/batch)\n",
            "2021-04-20 20:31:05: step 7280/50000, loss = 0.086110 (0.034 sec/batch)\n",
            "2021-04-20 20:31:06: step 7300/50000, loss = 0.058937 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7300: train_loss = 0.070289, dev_score = 0.919665\n",
            "Preds: [('DET', 'Her'), ('NOUN', 'yer'), ('NOUN', '<UNK>'), ('NOUN', '<UNK>'), ('ADJ', 'dolu'), ('PUNCT', '.')]\n",
            "2021-04-20 20:31:09: step 7320/50000, loss = 0.099716 (0.030 sec/batch)\n",
            "2021-04-20 20:31:10: step 7340/50000, loss = 0.053985 (0.033 sec/batch)\n",
            "2021-04-20 20:31:11: step 7360/50000, loss = 0.066857 (0.030 sec/batch)\n",
            "2021-04-20 20:31:12: step 7380/50000, loss = 0.079537 (0.028 sec/batch)\n",
            "2021-04-20 20:31:13: step 7400/50000, loss = 0.060199 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7400: train_loss = 0.069081, dev_score = 0.922951\n",
            "Preds: [('NOUN', '<UNK>'), ('VERB', '<UNK>'), ('PUNCT', '.')]\n",
            "2021-04-20 20:31:16: step 7420/50000, loss = 0.063037 (0.032 sec/batch)\n",
            "2021-04-20 20:31:17: step 7440/50000, loss = 0.057534 (0.035 sec/batch)\n",
            "2021-04-20 20:31:18: step 7460/50000, loss = 0.043275 (0.038 sec/batch)\n",
            "2021-04-20 20:31:19: step 7480/50000, loss = 0.070380 (0.036 sec/batch)\n",
            "2021-04-20 20:31:20: step 7500/50000, loss = 0.076317 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7500: train_loss = 0.070762, dev_score = 0.922376\n",
            "Preds: [('PRON', '<UNK>'), ('NOUN', 'suya'), ('VERB', 'atlayınca'), ('ADV', 'sanki'), ('ADV', 'kendinden'), ('VERB', 'geçti'), ('PUNCT', '.')]\n",
            "2021-04-20 20:31:23: step 7520/50000, loss = 0.069795 (0.031 sec/batch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3c8b9483ffa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-ed507e7b1524>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch, eval)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bjvrTcQYKHS"
      },
      "source": [
        "### Task 4. Error Analysis (1 point)\n",
        "\n",
        "Let's evaluate the model on the test set. First you need to load in the testing data. Create test_doc, test_dataset and test_loader. Evaluate the trainer on the test set.\n",
        "\n",
        "\n",
        "Create a confusion matrix, display it in readable format.\n",
        "Now, when we have the confusion matrix, let's calculate accuracy for each POS tag separately. \n",
        "\n",
        "Lastly, look at the confusion matrix and accuracy for each POS tag and describe what issues you can see. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_znTbidq-3o"
      },
      "source": [
        "test_doc = Document(DATA_PATH / 'tr_boun-ud-test.conllu')\n",
        "test_dataset = CONLLUDataset(test_doc, pretrain, vocab=vocab, test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y7u9tl0rO6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718e8cd9-df88-4774-df0b-620d5c09a5b7"
      },
      "source": [
        "test_preds = []\n",
        "test_words = []\n",
        "test_golds = []\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "for batch in test_loader:\n",
        "    batch_size = batch[0].size(0)\n",
        "    preds, gold = trainer.predict(batch)\n",
        "    test_golds += gold\n",
        "    test_correct += sum([1 for sent in zip(preds, gold) for pair in zip(*sent) if pair[0] == pair[1]])\n",
        "    test_total += sum([len(sent) for sent in gold])\n",
        "    test_preds += preds\n",
        "    pred_sents = [[batch[0][i][j] for j in range(batch[4][i])] for i in range(batch_size)]\n",
        "    test_words += [vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]]\n",
        "test_score = test_correct / test_total\n",
        "print(test_score)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9223396411894814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNT8I-6OH0t7",
        "outputId": "36fdc164-14ff-4311-a1d7-e73c7cab3a16"
      },
      "source": [
        "# Create the confusion matrix, you can use the sklearn confusion matrix\n",
        "\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "test_golds_binarized = mlb.fit_transform(test_golds)\n",
        "classes = mlb.classes_\n",
        "\n",
        "test_preds_binarized = mlb.fit_transform(test_preds)\n",
        "\n",
        "for i in range(len(classes)):\n",
        "  print('Confusion Matrix for POS-tag:', classes[i])\n",
        "  print()\n",
        "  print(multilabel_confusion_matrix(test_golds_binarized, test_preds_binarized)[i])\n",
        "  print('-'*50)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for POS-tag: ADJ\n",
            "\n",
            "[[352  44]\n",
            " [ 48 535]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: ADP\n",
            "\n",
            "[[733  14]\n",
            " [ 13 219]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: ADV\n",
            "\n",
            "[[544  36]\n",
            " [ 79 320]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: AUX\n",
            "\n",
            "[[812  14]\n",
            " [  3 150]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: CCONJ\n",
            "\n",
            "[[630   6]\n",
            " [  7 336]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: DET\n",
            "\n",
            "[[580  21]\n",
            " [  6 372]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: INTJ\n",
            "\n",
            "[[964   4]\n",
            " [  4   7]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: NOUN\n",
            "\n",
            "[[ 41  10]\n",
            " [ 12 916]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: NUM\n",
            "\n",
            "[[762  17]\n",
            " [ 31 169]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: PRON\n",
            "\n",
            "[[706  23]\n",
            " [ 22 228]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: PROPN\n",
            "\n",
            "[[694  20]\n",
            " [ 32 233]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: PUNCT\n",
            "\n",
            "[[ 46   0]\n",
            " [  0 933]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: VERB\n",
            "\n",
            "[[129  10]\n",
            " [ 10 830]]\n",
            "--------------------------------------------------\n",
            "Confusion Matrix for POS-tag: _\n",
            "\n",
            "[[874   2]\n",
            " [  1 102]]\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTdqXW8_ZGS"
      },
      "source": [
        "For this task I resorted to use the multilabel confusion matrix above, which doesn't present the overall picture, but rather shows the results on a label by label manner. Unfortunately confusion matrix function doesn't accept labels, it wants them in binary format. But even after binarizing the labels the matrix I got had less number of labels than we have used, so something must be wrong with what I did. I will still leave my code for you to see it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR82zLpiBc6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202bb669-b8d5-4ffb-a432-f3de6173f87f"
      },
      "source": [
        "# Create the confusion matrix, you can use the sklearn confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "test_golds_binarized = mlb.fit_transform(test_golds)\n",
        "classes = mlb.classes_\n",
        "\n",
        "test_preds_binarized = mlb.fit_transform(test_preds)\n",
        "\n",
        "\n",
        "print(confusion_matrix(y_true = test_golds_binarized.argmax(axis=1), y_pred = test_preds_binarized.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[531  11  23   5   1   4   0   8   0   0   0   0]\n",
            " [ 13  56   2   0   0   0   0   0   0   0   0   0]\n",
            " [ 16   4  71   0   4   3   0   2   0   0   1   0]\n",
            " [  2   0   2  12   0   0   0   0   0   0   0   0]\n",
            " [  3   0   7   0  25   0   0   0   0   0   0   0]\n",
            " [  4   0   6   0   0  17   0   0   0   0   0   0]\n",
            " [  0   0   1   0   0   0   1   0   0   0   0   0]\n",
            " [  6   2   8   0   1   1   0 101   1   1   3   0]\n",
            " [  0   0   0   0   0   0   0   1   2   0   0   0]\n",
            " [  0   0   0   0   0   0   0   1   0   5   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   4   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqjzSnQbYJi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29969fb5-bf1d-4e4d-f99a-71fbfa9cd9b4"
      },
      "source": [
        "# Calculate accuracy for each POS tag separately using confusion matrix\n",
        "for i in range(len(classes)):\n",
        "  print('Accuracy for POS-tag:', classes[i])\n",
        "  print()\n",
        "  tp_plus_tn = multilabel_confusion_matrix(test_golds_binarized, test_preds_binarized)[i][0][0] + multilabel_confusion_matrix(test_golds_binarized, test_preds_binarized)[i][1][1]\n",
        "  denominator = multilabel_confusion_matrix(test_golds_binarized, test_preds_binarized)[i][0][0] + multilabel_confusion_matrix(test_golds_binarized, test_preds_binarized)[i][0][1] + multilabel_confusion_matrix(test_golds_binarized, test_preds_binarized)[i][1][0] + multilabel_confusion_matrix(test_golds_binarized, test_preds_binarized)[i][1][1]\n",
        "  acc = round((tp_plus_tn / denominator)*100, 3)\n",
        "  print(acc)\n",
        "  print('-'*50)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for POS-tag: ADJ\n",
            "\n",
            "90.603\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: ADP\n",
            "\n",
            "97.242\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: ADV\n",
            "\n",
            "88.253\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: AUX\n",
            "\n",
            "98.264\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: CCONJ\n",
            "\n",
            "98.672\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: DET\n",
            "\n",
            "97.242\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: INTJ\n",
            "\n",
            "99.183\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: NOUN\n",
            "\n",
            "97.753\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: NUM\n",
            "\n",
            "95.097\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: PRON\n",
            "\n",
            "95.403\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: PROPN\n",
            "\n",
            "94.688\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: PUNCT\n",
            "\n",
            "100.0\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: VERB\n",
            "\n",
            "97.957\n",
            "--------------------------------------------------\n",
            "Accuracy for POS-tag: _\n",
            "\n",
            "99.694\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAexkx8mF8Wq"
      },
      "source": [
        "__What were the issues that you can see from the confusion matrix and calculated accuracies?__\n",
        "\n",
        "<font color='red'>Other than adverbs, the tagger has an accuracy bigger than 90% for all other labels. So the results are quite satisfactory for me, though I don't know what we are benchmarking against. Still, I am somewhat certain that these accuracies are comparable to human predictions, though may fall behind what experts would predict.\n",
        "\n",
        "One thing I suspect is that the tagger might have confused adverbs to adjectives, and adjectives to adverbs more frequently as compared to other false predictions for both of those tags. This may be due to the fact that both of these tags are similar in terms of their function in  a sentence, ie they are qualifiers. So even if one qualifies nouns while the other one is used for verbs, the tagger might have been confused while separating them from each other.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIVnti9di2k9"
      },
      "source": [
        "## Task 5. Propose a new approach (2 points)\n",
        "\n",
        "So far, you should have an idea on how the sequence tagging is done. Now you will have to propose an approach for a new problem.\n",
        "\n",
        "Imagine that you need to add morphological tagging to your model. The tags are stored in the [_feats_ field](https://universaldependencies.org/format.html#morphological-annotation) of the CoNLL-U format. From the official description, thay have the following format:\n",
        "\n",
        "> The FEATS field contains a list of morphological features, with vertical bar (|) as list separator and with underscore to represent the empty list. All features should be represented as attribute-value pairs, with an equals sign (=) separating the attribute from the value.\n",
        "\n",
        "Your task is to describe how you will modify the current POS tagger model to include FEATS tagger as well. Your description must answer the following questions:\n",
        "\n",
        "- Should you add a new vocab to read the feats? How will you build this vocab?\n",
        "- POS tagging can be treated as a multi-class classification task, i.e. we assign each word to exaclty one POS tag (which is a class in our situation). What kind of task is morphological tagging (multi-label, binary classification, multiple multi-class tasks)?\n",
        "- For example, nouns have case and gender attributes but verbs don't have them. How are you going to tackle this?\n",
        "- Which new layers are you going to introduce into the model?\n",
        "- What metrics are you going to use to measure the performance of morphological tagging?\n",
        "\n",
        "You can also try to visualize the model to make it easier to see your concept.\n",
        "\n",
        "<font color='red'>\n",
        "\n",
        "1. Yes we should since we need to store the original morphological tags for every word. We already get the two fields, text and upos, under the load_doc method of ConlluDataset class. We just need to add the feats field to those so that we will load the morphological tags for our words with the conlludataset class. After this we need to alter the init_vocab and preprocess methods accordingly, ie we need to use Wordvocab with idx=2 to create the vocabulary for the morphological tags and then preprocess them with the latter method.\n",
        "\n",
        "2. Morph tagging is a multi-label classification task since a word can have more than one morphological tag, unlike POS-tagging, where every word can have at most one POS tag.\n",
        "\n",
        "3. I am not sure about my answer, but i propose to predict the pos-tag first, and then after getting such predictions, predict the morphological tags by using both the word and the part-of-speech tag of the word. This would ensure the model to understand that it is dealing with a noun, verb or an adverb.\n",
        "\n",
        "  One can also go by splitting up the morphological tags by using a regex and fetch the values such as 'Nom', 'Plur', 'Past', 'Pres', 'None'(or '-') etc from the morph tag attributes. The prediction task becomes more granular this way and we will output a long list of predictions, with each element of the list stands for a morph tag attribute value plus the pos-tag. My problem with this more detailed approach is that I couldn't imagine how to construct the vocabulary that will be needed. It will be more complex to split up morph tags using a form of regex that will then be fed into morphological vocab class. Also, I am not exactly sure if doing this will bring significantly better results as compared to batch prediction of all morphological features, so I decided not to tinker much.\n",
        "\n",
        "4. I have hinted the model architecture above, but to be more specific, I think that after fetching the UPOS value, we need to encode it to one-hot representation and create another BiLSTM model that takes word&character embeddings besides the one-hot encoded POS-tags for predicting the morphological tags. It will be better to add linear layers after the BiLSTM for returning the output, but I don't know how many of them we would need really.\n",
        "\n",
        "5. We need to use f1-score for understanding the performance of the second tagger, since some of the morphological tags may be in minority as compared to others. I will use the same loss as in the first tagger, ie categorical cross entropy.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcGoPgJLYO_U"
      },
      "source": [
        "### Bonus. Hidden Markov Model for POS tagging (1 point)\n",
        "\n",
        "Hidden Markov Model (HMM) is a probabilistic model, for POS tagging it will take as an input a sequence of words, computes a possible sequences of POS tags from it and then chooses the best sequence of tags. \n",
        "\n",
        "The POS tags cannot be observed directly (they are hidden), we only see the words and infer the tags from the sequence. \n",
        "\n",
        "HMM is defined with the following components: \n",
        "\n",
        "$ Q = Q_1q_2...q_N $ - a set of N states <br>\n",
        "$ A = a_{11}...a_{ij}...a_{NN}$ - transition probability matrix, each $a_{ij}$ represents the probability of moving from state i to state j.  <br>\n",
        "$O=o_1o_2...o_T$ - a sequence of T observations <br>\n",
        "$B=b_i(o_t)$ - sequence of observation likelihoods (emission probabilities), each expresses the probability of an observation $o_t$ being generated from a state $q_i$  <br>\n",
        "$\\pi=\\pi_1,\\pi_2,...,\\pi_N$ - initial probability distribution over states. $+\\pi_i£ is the probability that the Markov chain will start in state i. \n",
        "\n",
        "We also need to know the two simplifying assumptions:\n",
        "\n",
        "1) The probability of a particular state depends only on the previous state: <br>\n",
        " Makrov Assumption: $$P(q_i|q_1,...q_{i-1})=P(q_i|q_{i-1})$$\n",
        "\n",
        "2) The probability of an output observation $o_i$ depends only on the state that produced the observation $q_i$ and not on any other states or any other observations: <br>\n",
        "Output Independence: $$P(o_i|q_1,...q_i,...q_T,o_1,...o_i,...o_T)=P(o_i|q_i)$$\n",
        "\n",
        "\n",
        "HMM itself has two components: transition probabilities and emission probabilities. \n",
        "\n",
        "We can calculate the maximum likelihood estimate for transition probability by counting out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second. <br>\n",
        "\n",
        "$$P(t_i|t_{i-1}) = \\frac{C(t_{i-1}, t_i)}{C(t_{i-1})}$$\n",
        "\n",
        "\n",
        "We can calculate the emission probabilities by counting how many times we see the tag in the corpus and how many times this tag is assigned to a specific word:\n",
        "\n",
        "$$P(w_i|t_i) = \\frac{C(t_i, w_i)}{C(t_i)}  $$\n",
        "\n",
        "You can read more about the HMM from the Dainel Jurafsky & James H. Martin [book](https://web.stanford.edu/~jurafsky/slp3/8.pdf) called Speech and Language Processing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unozGBA9MHaT"
      },
      "source": [
        "import nltk \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import random\n",
        "!pip install conllu\n",
        "from conllu import parse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ARgo8F0qc9"
      },
      "source": [
        "Read in the UD data that you have already downloaded earlier. Replace the path to train and test dataset with your own downloaded splits. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG92XBVpnqe8"
      },
      "source": [
        "train_path = Path('data') / ... / ...\n",
        "test_path = Path('data') / ... / ... \n",
        "\n",
        "train_data = open(train_path, 'r', encoding='utf-8').read()\n",
        "train_sentences = parse(train_data)\n",
        "train_set = [[(token.get('form'),token.get('upos')) for token in sentence] for sentence in train_sentences ]\n",
        "\n",
        "test_data = open(test_path, 'r', encoding='utf-8').read()\n",
        "test_sentences = parse(test_data)\n",
        "test_set = [[(token.get('form'),token.get('upos')) for token in sentence ] for sentence in test_sentences]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UEny_HPot7X"
      },
      "source": [
        "train_tagged = [ pair for pair in sent for sent in train_set ]\n",
        "test_tagged = [ pair for pair in sent for sent in test_set ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYTBlUZ8pOmz"
      },
      "source": [
        "pos_tags = list({tag for _,tag in train_tagged})\n",
        "print(pos_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUry7eGv1ul-"
      },
      "source": [
        "Now, we have the tagged sentences, the POS tags and vocabulary. \n",
        "Next, we need to calculate emission and transition probabilities. Use the formulas given above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-GMzFDYmlyG"
      },
      "source": [
        "def emission_probability(word, tag, train=train_tagged):\n",
        "  c_t1 = ... # count how many times this tag occurs in the dataset\n",
        "  c_tiwi = ... # count how many times this tag has been associated with the given word\n",
        "  p_witi = ... # calculate the emission probability \n",
        "  return p_witi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFY_xkowmoaF"
      },
      "source": [
        "def transition_probability(tag1, tag2, train = train_tagged):\n",
        "    tags = ... # collect all the pos tags from the train\n",
        "    c_ti = ... # count all the times we see tag1 in the corpus \n",
        "    c_ti_ti1 = 0\n",
        "    for index in range(len(tags)-1):\n",
        "        if ...:  # count the times we see tag1 before tag2\n",
        "            c_ti_ti1  += 1\n",
        "    p_titi1 = ... # calculate the transition_probability\n",
        "    return p_titi1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHdcSlHB7Jkp"
      },
      "source": [
        "Now, let's create a transition matrix: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO0lBPInmtYk"
      },
      "source": [
        "transition_matrix = np.zeros((len(pos_tags), len(pos_tags)), dtype='float32')\n",
        "for i, tag1 in enumerate(list(pos_tags)):\n",
        "    for j, tag2 in enumerate(list(pos_tags)): \n",
        "        transition_matrix[i, j] = ... # assign the correct transition probability (hint: call the transition_probability function)\n",
        " \n",
        "df_tags = pd.DataFrame(transition_matrix, columns=list(pos_tags), index=list(pos_tags))\n",
        "display(df_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqwTjHod2c0T"
      },
      "source": [
        "To decode the sequence from HMM we can use Viterbi algorithm, which is a dynamical programming algorithm. For POS tagging, we need to find the most probable tag sequence given the observation sequence of n words. \n",
        "\n",
        "There are two assumptions made while decoding tag sequence: \n",
        "\n",
        "1) The probability of a word appearing depends only on its own tag and is independent of neighboring words and tags. \n",
        "\n",
        "2) The probability of a tag depends only on the previous tag. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUJ7qSUIjbN_"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/540/1*8-5KZVj-_jZOWN83gGhD5A.png\" >\n",
        "Image from Medium post *POS Tagging using Hidden Markov Models (HMM) & Viterbi algorithm in NLP mathematics explained* by Mehul Gupta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9pQpZmj5Rl"
      },
      "source": [
        "First, we need to define probability matrix called lattice (let's define it as V). Each row represents a POS tag (hidden state) and each column represents a word (observables). Look at the image above to get an idea of how this matrix looks. We need to fill in this matrix by calculating the probabilities for each cell, for example for $V_{jt}$, we have to calculate the probability that the HMM is in some state j after seeing the first t observations. The $V_{jt}$ can be calculated: \n",
        "$$V_{jt}= max(V_{t-1}*a[i,j]*b_j(o_t))$$\n",
        "Here, $v_{t-1}(i)$ is the previous Viterbi path probability from the previous time step. $a_{ij}$ is the transition probability from previous state $q_i$ to current state $q_j$. $b_j(o_t)$ is the state observation likelihood of the observation symbol $o_t$ given the current state j. \n",
        "\n",
        "\n",
        "Your task regarding the Viterbi algorithm is to just fill in the slots in the function below. You need to take the correct transition probability given the previous tag and current tag from the df_tags matrix. You can access the previous tag from the states list. \n",
        "\n",
        "Next, you need to calculate the emission probability given the word and the tag. \n",
        "\n",
        "After all the probabilities for each POS for the word have been calculated, we will be selecting the maximum probability from the probabilities list and add the best state (POS tag) to the states list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMl6oq34mzAL"
      },
      "source": [
        "def Viterbi(words,tags, transition_matrix=df_tags):\n",
        "    states = [] # we are saving the best states only \n",
        "    for key, word in enumerate(words): # iterating over the observations\n",
        "        probabilities = [] # saving the probabilities \n",
        "        for tag in tags: # iterating over the observations (we are filling one column)\n",
        "            if key == 0: # if the word is the first one in the sequence\n",
        "                transition_prob = df_tags.loc['PUNCT', tag]\n",
        "            else:\n",
        "                transition_prob = ... \n",
        "            \n",
        "            emission_prob = ...\n",
        "            state_prob = emission_prob * transition_prob \n",
        "            probabilities.append(state_prob)\n",
        "             \n",
        "        pmax = ... # take the maximum value from the probabilities list\n",
        "        state_max = ... # get the state so the probability is maximum (hint: get the index where this maximum probability was in the probabilities list and use it to get the tag from the tags list)\n",
        "        states.append(state_max)\n",
        "    return list(zip(words, states))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTlvbQvwuvRC"
      },
      "source": [
        "Let's test the algorithm. Let's choose 15 sentences from the test set and calculate the accuracy on this set. If you want you can choose bigger sample. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nni672gfnc6v"
      },
      "source": [
        "random_set = [random.randint(1,len(test_set)-1) for x in range(15)]\n",
        " \n",
        "test = [test_set[i] for i in random_set]\n",
        "test_tags = [tup[1] for sent in test for tup in sent]\n",
        "test_words = [tup[0] for sent in test for tup in sent]\n",
        "tagged_seq  = Viterbi(test_words, list(pos_tags))\n",
        "\n",
        "check = [pred  for pred, gold in zip(tagged_seq, test_tags) if pred[1] == gold] \n",
        " \n",
        "accuracy = len(check)/len(tagged_seq)\n",
        "print('Accuracy for a small subset: ',accuracy*100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}